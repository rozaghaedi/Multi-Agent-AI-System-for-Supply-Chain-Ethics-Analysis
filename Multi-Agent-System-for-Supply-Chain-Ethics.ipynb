{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Multi-agent System**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **DataSet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T05:41:31.791166Z",
     "iopub.status.busy": "2025-12-01T05:41:31.790879Z",
     "iopub.status.idle": "2025-12-01T05:41:34.929588Z",
     "shell.execute_reply": "2025-12-01T05:41:34.928306Z",
     "shell.execute_reply.started": "2025-12-01T05:41:31.791134Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: gdown\n"
     ]
    }
   ],
   "source": [
    "!gdown --id 16AahMNlkBkBsawCF6yJPqnQf7FNMc48B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T05:41:38.309694Z",
     "iopub.status.busy": "2025-12-01T05:41:38.309088Z",
     "iopub.status.idle": "2025-12-01T05:41:49.292851Z",
     "shell.execute_reply": "2025-12-01T05:41:49.291746Z",
     "shell.execute_reply.started": "2025-12-01T05:41:38.309644Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "conda-repo-cli 1.0.75 requires requests_mock, which is not installed.\n",
      "streamlit 1.30.0 requires packaging<24,>=16.8, but you have packaging 25.0 which is incompatible.\n",
      "streamlit 1.30.0 requires protobuf<5,>=3.20, but you have protobuf 6.33.1 which is incompatible.\n",
      "streamlit 1.30.0 requires tenacity<9,>=8.1.0, but you have tenacity 9.1.2 which is incompatible.\n",
      "conda-repo-cli 1.0.75 requires clyent==1.2.1, but you have clyent 1.2.2 which is incompatible.\n",
      "conda-repo-cli 1.0.75 requires python-dateutil==2.8.2, but you have python-dateutil 2.9.0.post0 which is incompatible.\n",
      "conda-repo-cli 1.0.75 requires PyYAML==6.0.1, but you have pyyaml 6.0.3 which is incompatible.\n",
      "conda-repo-cli 1.0.75 requires requests==2.31.0, but you have requests 2.32.5 which is incompatible.\n",
      "anaconda-cloud-auth 0.1.4 requires pydantic<2.0, but you have pydantic 2.12.5 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m<frozen runpy>:128: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/roza.ghaedi/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "!pip install google-adk pandas scikit-learn numpy nltk -q\n",
    "!python -m nltk.downloader vader_lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T05:41:49.295405Z",
     "iopub.status.busy": "2025-12-01T05:41:49.295003Z",
     "iopub.status.idle": "2025-12-01T05:41:50.656105Z",
     "shell.execute_reply": "2025-12-01T05:41:50.655081Z",
     "shell.execute_reply.started": "2025-12-01T05:41:49.295360Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sheet1 = pd.read_excel(\"/kaggle/working/KTC-2025-ICT-benchmark-data.xlsx\", sheet_name=\"1) Scoring\")\n",
    "sheet2 = pd.read_excel(\"/kaggle/working/KTC-2025-ICT-benchmark-data.xlsx\", sheet_name=\"2) Detailed scoring & research\")\n",
    "sheet3 = pd.read_excel(\"/kaggle/working/KTC-2025-ICT-benchmark-data.xlsx\", sheet_name=\"3) Non-scored research\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T05:41:50.657822Z",
     "iopub.status.busy": "2025-12-01T05:41:50.657447Z",
     "iopub.status.idle": "2025-12-01T05:41:50.702092Z",
     "shell.execute_reply": "2025-12-01T05:41:50.701083Z",
     "shell.execute_reply.started": "2025-12-01T05:41:50.657792Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n",
      "  has_large_values = (abs_vals > 1e6).any()\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n",
      "  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n",
      "  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n",
      "  has_large_values = (abs_vals > 1e6).any()\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n",
      "  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n",
      "  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company ID</th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Year of inclusion</th>\n",
       "      <th>Country</th>\n",
       "      <th>Region</th>\n",
       "      <th>Subindustry</th>\n",
       "      <th>Market cap</th>\n",
       "      <th>1. Supplier Code of Conduct and Capacity Building</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "      <th>...</th>\n",
       "      <th>Unnamed: 51</th>\n",
       "      <th>Unnamed: 52</th>\n",
       "      <th>Unnamed: 53</th>\n",
       "      <th>Unnamed: 54</th>\n",
       "      <th>Unnamed: 55</th>\n",
       "      <th>Unnamed: 56</th>\n",
       "      <th>Unnamed: 57</th>\n",
       "      <th>Unnamed: 58</th>\n",
       "      <th>Unnamed: 59</th>\n",
       "      <th>Unnamed: 60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Indicator Number</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>Total</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Total benchmark score</td>\n",
       "      <td>2025 Rank</td>\n",
       "      <td>Commitment &amp; Governance</td>\n",
       "      <td>Traceability &amp; Risk Assessment</td>\n",
       "      <td>Purchasing Practices</td>\n",
       "      <td>Recruitment</td>\n",
       "      <td>Enabling Workers' Rights</td>\n",
       "      <td>Monitoring</td>\n",
       "      <td>Remedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Company ID Company Name  Year of inclusion Country Region  \\\n",
       "0  Indicator Number          NaN                NaN     NaN    NaN   \n",
       "1               NaN          NaN                NaN     NaN    NaN   \n",
       "\n",
       "  Subindustry  Market cap  1. Supplier Code of Conduct and Capacity Building  \\\n",
       "0         NaN         NaN                                                1.1   \n",
       "1         NaN         NaN                                                NaN   \n",
       "\n",
       "   Unnamed: 8 Unnamed: 9  ...  Unnamed: 51            Unnamed: 52  \\\n",
       "0         1.2      Total  ...          NaN                    NaN   \n",
       "1         NaN        NaN  ...          NaN  Total benchmark score   \n",
       "\n",
       "   Unnamed: 53              Unnamed: 54                     Unnamed: 55  \\\n",
       "0          NaN                      NaN                             NaN   \n",
       "1    2025 Rank  Commitment & Governance  Traceability & Risk Assessment   \n",
       "\n",
       "            Unnamed: 56  Unnamed: 57               Unnamed: 58  Unnamed: 59  \\\n",
       "0                   NaN          NaN                       NaN          NaN   \n",
       "1  Purchasing Practices  Recruitment  Enabling Workers' Rights   Monitoring   \n",
       "\n",
       "   Unnamed: 60  \n",
       "0          NaN  \n",
       "1       Remedy  \n",
       "\n",
       "[2 rows x 61 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sheet1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T05:41:54.846496Z",
     "iopub.status.busy": "2025-12-01T05:41:54.846122Z",
     "iopub.status.idle": "2025-12-01T05:41:54.864650Z",
     "shell.execute_reply": "2025-12-01T05:41:54.863870Z",
     "shell.execute_reply.started": "2025-12-01T05:41:54.846470Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 1</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Commitment &amp; Governance</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "      <th>...</th>\n",
       "      <th>Unnamed: 78</th>\n",
       "      <th>Unnamed: 79</th>\n",
       "      <th>Unnamed: 80</th>\n",
       "      <th>Unnamed: 81</th>\n",
       "      <th>Unnamed: 82</th>\n",
       "      <th>Unnamed: 83</th>\n",
       "      <th>Unnamed: 84</th>\n",
       "      <th>Unnamed: 85</th>\n",
       "      <th>Unnamed: 86</th>\n",
       "      <th>Unnamed: 87</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1. Supplier Code of Conduct and Implementation...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2. Management and Accountability\\nThe company ...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Company</td>\n",
       "      <td>Year of inclusion</td>\n",
       "      <td>Country</td>\n",
       "      <td>Region</td>\n",
       "      <td>Market cap (USD)</td>\n",
       "      <td>1.1  has a supplier code of conduct that requi...</td>\n",
       "      <td>1.2 engages in capacity building to enable its...</td>\n",
       "      <td>Comment</td>\n",
       "      <td>Source(s)</td>\n",
       "      <td>2.1 has a committee, team, program, or officer...</td>\n",
       "      <td>...</td>\n",
       "      <td>B.1.1 that it engages in a dialogue with the s...</td>\n",
       "      <td>B.1.2 outcomes of the remedy process in the ca...</td>\n",
       "      <td>B.1.3 evidence that remedy(ies) are satisfacto...</td>\n",
       "      <td>Comment</td>\n",
       "      <td>Source(s)</td>\n",
       "      <td>Allegation Summary (Allegation Denied)</td>\n",
       "      <td>B.2.1 a description of what actions it would t...</td>\n",
       "      <td>B.2.2  that it engages in a dialogue with the ...</td>\n",
       "      <td>Comment Text</td>\n",
       "      <td>Source(s)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 88 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0         Unnamed: 1 Unnamed: 2 Unnamed: 3        Unnamed: 4  \\\n",
       "0        NaN                NaN        NaN        NaN               NaN   \n",
       "1    Company  Year of inclusion    Country     Region  Market cap (USD)   \n",
       "\n",
       "                             Commitment & Governance  \\\n",
       "0  1. Supplier Code of Conduct and Implementation...   \n",
       "1  1.1  has a supplier code of conduct that requi...   \n",
       "\n",
       "                                          Unnamed: 6 Unnamed: 7 Unnamed: 8  \\\n",
       "0                                                NaN        NaN        NaN   \n",
       "1  1.2 engages in capacity building to enable its...    Comment  Source(s)   \n",
       "\n",
       "                                          Unnamed: 9  ...  \\\n",
       "0  2. Management and Accountability\\nThe company ...  ...   \n",
       "1  2.1 has a committee, team, program, or officer...  ...   \n",
       "\n",
       "                                         Unnamed: 78  \\\n",
       "0                                                NaN   \n",
       "1  B.1.1 that it engages in a dialogue with the s...   \n",
       "\n",
       "                                         Unnamed: 79  \\\n",
       "0                                                NaN   \n",
       "1  B.1.2 outcomes of the remedy process in the ca...   \n",
       "\n",
       "                                         Unnamed: 80 Unnamed: 81 Unnamed: 82  \\\n",
       "0                                                NaN         NaN         NaN   \n",
       "1  B.1.3 evidence that remedy(ies) are satisfacto...     Comment   Source(s)   \n",
       "\n",
       "                              Unnamed: 83  \\\n",
       "0                                     NaN   \n",
       "1  Allegation Summary (Allegation Denied)   \n",
       "\n",
       "                                         Unnamed: 84  \\\n",
       "0                                                NaN   \n",
       "1  B.2.1 a description of what actions it would t...   \n",
       "\n",
       "                                         Unnamed: 85   Unnamed: 86 Unnamed: 87  \n",
       "0                                                NaN           NaN         NaN  \n",
       "1  B.2.2  that it engages in a dialogue with the ...  Comment Text   Source(s)  \n",
       "\n",
       "[2 rows x 88 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sheet2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T05:41:54.942867Z",
     "iopub.status.busy": "2025-12-01T05:41:54.942588Z",
     "iopub.status.idle": "2025-12-01T05:41:54.960143Z",
     "shell.execute_reply": "2025-12-01T05:41:54.959120Z",
     "shell.execute_reply.started": "2025-12-01T05:41:54.942845Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Overview of Company Information</th>\n",
       "      <th>Unnamed: 1</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "      <th>Compliance with Regulatory Requirements</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "      <th>...</th>\n",
       "      <th>Unnamed: 13</th>\n",
       "      <th>Unnamed: 14</th>\n",
       "      <th>Unnamed: 15</th>\n",
       "      <th>Unnamed: 16</th>\n",
       "      <th>Unnamed: 17</th>\n",
       "      <th>Unnamed: 18</th>\n",
       "      <th>Unnamed: 19</th>\n",
       "      <th>Unnamed: 20</th>\n",
       "      <th>Unnamed: 21</th>\n",
       "      <th>Unnamed: 22</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UK Modern Slavery Act</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Australia Modern Slavery Act</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sourcing from High-Risk Countries</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Company</td>\n",
       "      <td>Year of inclusion</td>\n",
       "      <td>Country</td>\n",
       "      <td>Region</td>\n",
       "      <td>Market Cap</td>\n",
       "      <td>Ticker</td>\n",
       "      <td>ISIN</td>\n",
       "      <td>Subindustry</td>\n",
       "      <td>The company (or its subsidiary) is required to...</td>\n",
       "      <td>The company has published a statement</td>\n",
       "      <td>...</td>\n",
       "      <td>The company has published a statement</td>\n",
       "      <td>Comment</td>\n",
       "      <td>Source</td>\n",
       "      <td>The company is required to report</td>\n",
       "      <td>The company has published a statement</td>\n",
       "      <td>Comment</td>\n",
       "      <td>Source</td>\n",
       "      <td>China</td>\n",
       "      <td>Malaysia</td>\n",
       "      <td>Source</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Overview of Company Information         Unnamed: 1 Unnamed: 2 Unnamed: 3  \\\n",
       "0                             NaN                NaN        NaN        NaN   \n",
       "1                         Company  Year of inclusion    Country     Region   \n",
       "\n",
       "   Unnamed: 4 Unnamed: 5 Unnamed: 6   Unnamed: 7  \\\n",
       "0         NaN        NaN        NaN          NaN   \n",
       "1  Market Cap     Ticker       ISIN  Subindustry   \n",
       "\n",
       "             Compliance with Regulatory Requirements  \\\n",
       "0                              UK Modern Slavery Act   \n",
       "1  The company (or its subsidiary) is required to...   \n",
       "\n",
       "                               Unnamed: 9  ...  \\\n",
       "0                                     NaN  ...   \n",
       "1  The company has published a statement   ...   \n",
       "\n",
       "                             Unnamed: 13 Unnamed: 14 Unnamed: 15  \\\n",
       "0                                    NaN         NaN         NaN   \n",
       "1  The company has published a statement     Comment      Source   \n",
       "\n",
       "                         Unnamed: 16                            Unnamed: 17  \\\n",
       "0       Australia Modern Slavery Act                                    NaN   \n",
       "1  The company is required to report  The company has published a statement   \n",
       "\n",
       "  Unnamed: 18 Unnamed: 19                        Unnamed: 20 Unnamed: 21  \\\n",
       "0         NaN         NaN  Sourcing from High-Risk Countries         NaN   \n",
       "1     Comment      Source                              China    Malaysia   \n",
       "\n",
       "  Unnamed: 22  \n",
       "0         NaN  \n",
       "1      Source  \n",
       "\n",
       "[2 rows x 23 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sheet3.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T05:41:55.189802Z",
     "iopub.status.busy": "2025-12-01T05:41:55.189502Z",
     "iopub.status.idle": "2025-12-01T05:41:55.217127Z",
     "shell.execute_reply": "2025-12-01T05:41:55.216270Z",
     "shell.execute_reply.started": "2025-12-01T05:41:55.189773Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 49 entries, 0 to 48\n",
      "Data columns (total 61 columns):\n",
      " #   Column                                             Non-Null Count  Dtype  \n",
      "---  ------                                             --------------  -----  \n",
      " 0   Company ID                                         46 non-null     object \n",
      " 1   Company Name                                       45 non-null     object \n",
      " 2   Year of inclusion                                  45 non-null     float64\n",
      " 3   Country                                            45 non-null     object \n",
      " 4   Region                                             45 non-null     object \n",
      " 5   Subindustry                                        45 non-null     object \n",
      " 6   Market cap                                         45 non-null     float64\n",
      " 7   1. Supplier Code of Conduct and Capacity Building  46 non-null     float64\n",
      " 8   Unnamed: 8                                         46 non-null     float64\n",
      " 9   Unnamed: 9                                         46 non-null     object \n",
      " 10  2. Management and Accountability                   46 non-null     float64\n",
      " 11  Unnamed: 11                                        46 non-null     float64\n",
      " 12  Unnamed: 12                                        46 non-null     float64\n",
      " 13  Unnamed: 13                                        46 non-null     object \n",
      " 14  3. Traceability and Supply Chain Transparency      46 non-null     float64\n",
      " 15  Unnamed: 15                                        46 non-null     float64\n",
      " 16  Unnamed: 16                                        46 non-null     float64\n",
      " 17  Unnamed: 17                                        46 non-null     object \n",
      " 18  4. Risk Assessment                                 46 non-null     float64\n",
      " 19  Unnamed: 19                                        46 non-null     float64\n",
      " 20  Unnamed: 20                                        46 non-null     float64\n",
      " 21  Unnamed: 21                                        46 non-null     object \n",
      " 22  5. Data on Supply Chain Risks                      46 non-null     float64\n",
      " 23  Unnamed: 23                                        46 non-null     float64\n",
      " 24  Unnamed: 24                                        46 non-null     float64\n",
      " 25  Unnamed: 25                                        46 non-null     object \n",
      " 26  6. Purchasing Practices                            46 non-null     float64\n",
      " 27  Unnamed: 27                                        46 non-null     float64\n",
      " 28  Unnamed: 28                                        46 non-null     float64\n",
      " 29  Unnamed: 29                                        46 non-null     object \n",
      " 30  7. Recruitment Fees and Related Costs              46 non-null     float64\n",
      " 31  Unnamed: 31                                        46 non-null     float64\n",
      " 32  Unnamed: 32                                        46 non-null     float64\n",
      " 33  Unnamed: 33                                        46 non-null     object \n",
      " 34  8. Responsible Recruitment                         46 non-null     float64\n",
      " 35  Unnamed: 35                                        46 non-null     float64\n",
      " 36  Unnamed: 36                                        46 non-null     object \n",
      " 37  9. Freedom of Association                          46 non-null     float64\n",
      " 38  Unnamed: 38                                        46 non-null     float64\n",
      " 39  Unnamed: 39                                        46 non-null     float64\n",
      " 40  Unnamed: 40                                        46 non-null     object \n",
      " 41  10. Grievance Mechanism                            46 non-null     float64\n",
      " 42  Unnamed: 42                                        46 non-null     float64\n",
      " 43  Unnamed: 43                                        46 non-null     float64\n",
      " 44  Unnamed: 44                                        46 non-null     object \n",
      " 45  11. Monitoring                                     46 non-null     float64\n",
      " 46  Unnamed: 46                                        46 non-null     float64\n",
      " 47  Unnamed: 47                                        46 non-null     float64\n",
      " 48  Unnamed: 48                                        46 non-null     object \n",
      " 49  12. Remedy                                         46 non-null     float64\n",
      " 50  Unnamed: 50                                        46 non-null     float64\n",
      " 51  Unnamed: 51                                        45 non-null     float64\n",
      " 52  Unnamed: 52                                        46 non-null     object \n",
      " 53  Unnamed: 53                                        46 non-null     object \n",
      " 54  Unnamed: 54                                        47 non-null     object \n",
      " 55  Unnamed: 55                                        47 non-null     object \n",
      " 56  Unnamed: 56                                        47 non-null     object \n",
      " 57  Unnamed: 57                                        47 non-null     object \n",
      " 58  Unnamed: 58                                        47 non-null     object \n",
      " 59  Unnamed: 59                                        47 non-null     object \n",
      " 60  Unnamed: 60                                        47 non-null     object \n",
      "dtypes: float64(36), object(25)\n",
      "memory usage: 23.5+ KB\n"
     ]
    }
   ],
   "source": [
    "sheet1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T05:41:55.426132Z",
     "iopub.status.busy": "2025-12-01T05:41:55.425835Z",
     "iopub.status.idle": "2025-12-01T05:41:55.440567Z",
     "shell.execute_reply": "2025-12-01T05:41:55.439628Z",
     "shell.execute_reply.started": "2025-12-01T05:41:55.426108Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 47 entries, 0 to 46\n",
      "Data columns (total 88 columns):\n",
      " #   Column                            Non-Null Count  Dtype \n",
      "---  ------                            --------------  ----- \n",
      " 0   Unnamed: 0                        46 non-null     object\n",
      " 1   Unnamed: 1                        46 non-null     object\n",
      " 2   Unnamed: 2                        46 non-null     object\n",
      " 3   Unnamed: 3                        46 non-null     object\n",
      " 4   Unnamed: 4                        46 non-null     object\n",
      " 5   Commitment & Governance           47 non-null     object\n",
      " 6   Unnamed: 6                        46 non-null     object\n",
      " 7   Unnamed: 7                        46 non-null     object\n",
      " 8   Unnamed: 8                        46 non-null     object\n",
      " 9   Unnamed: 9                        47 non-null     object\n",
      " 10  Unnamed: 10                       46 non-null     object\n",
      " 11  Unnamed: 11                       46 non-null     object\n",
      " 12  Unnamed: 12                       46 non-null     object\n",
      " 13  Unnamed: 13                       46 non-null     object\n",
      " 14  Traceability and Risk Assessment  47 non-null     object\n",
      " 15  Unnamed: 15                       46 non-null     object\n",
      " 16  Unnamed: 16                       46 non-null     object\n",
      " 17  Unnamed: 17                       46 non-null     object\n",
      " 18  Unnamed: 18                       46 non-null     object\n",
      " 19  Unnamed: 19                       47 non-null     object\n",
      " 20  Unnamed: 20                       46 non-null     object\n",
      " 21  Unnamed: 21                       46 non-null     object\n",
      " 22  Unnamed: 22                       46 non-null     object\n",
      " 23  Unnamed: 23                       46 non-null     object\n",
      " 24  Unnamed: 24                       47 non-null     object\n",
      " 25  Unnamed: 25                       46 non-null     object\n",
      " 26  Unnamed: 26                       46 non-null     object\n",
      " 27  Unnamed: 27                       46 non-null     object\n",
      " 28  Unnamed: 28                       46 non-null     object\n",
      " 29  Purchasing Practices              47 non-null     object\n",
      " 30  Unnamed: 30                       46 non-null     object\n",
      " 31  Unnamed: 31                       46 non-null     object\n",
      " 32  Unnamed: 32                       46 non-null     object\n",
      " 33  Unnamed: 33                       46 non-null     object\n",
      " 34  Recruitment                       47 non-null     object\n",
      " 35  Unnamed: 35                       46 non-null     object\n",
      " 36  Unnamed: 36                       46 non-null     object\n",
      " 37  Unnamed: 37                       46 non-null     object\n",
      " 38  Unnamed: 38                       46 non-null     object\n",
      " 39  Unnamed: 39                       47 non-null     object\n",
      " 40  Unnamed: 40                       46 non-null     object\n",
      " 41  Unnamed: 41                       46 non-null     object\n",
      " 42  Unnamed: 42                       45 non-null     object\n",
      " 43  Enabling Workers' Rights          47 non-null     object\n",
      " 44  Unnamed: 44                       46 non-null     object\n",
      " 45  Unnamed: 45                       46 non-null     object\n",
      " 46  Unnamed: 46                       46 non-null     object\n",
      " 47  Unnamed: 47                       46 non-null     object\n",
      " 48  Unnamed: 48                       47 non-null     object\n",
      " 49  Unnamed: 49                       46 non-null     object\n",
      " 50  Unnamed: 50                       46 non-null     object\n",
      " 51  Unnamed: 51                       46 non-null     object\n",
      " 52  Unnamed: 52                       46 non-null     object\n",
      " 53  Monitoring                        47 non-null     object\n",
      " 54  Unnamed: 54                       46 non-null     object\n",
      " 55  Unnamed: 55                       46 non-null     object\n",
      " 56  Unnamed: 56                       46 non-null     object\n",
      " 57  Unnamed: 57                       46 non-null     object\n",
      " 58  Unnamed: 58                       47 non-null     object\n",
      " 59  Unnamed: 59                       46 non-null     object\n",
      " 60  Unnamed: 60                       46 non-null     object\n",
      " 61  Remedy                            46 non-null     object\n",
      " 62  Unnamed: 62                       46 non-null     object\n",
      " 63  Unnamed: 63                       46 non-null     object\n",
      " 64  Unnamed: 64                       45 non-null     object\n",
      " 65  Unnamed: 65                       37 non-null     object\n",
      " 66  Unnamed: 66                       37 non-null     object\n",
      " 67  Unnamed: 67                       37 non-null     object\n",
      " 68  Unnamed: 68                       37 non-null     object\n",
      " 69  Unnamed: 69                       37 non-null     object\n",
      " 70  Unnamed: 70                       24 non-null     object\n",
      " 71  Unnamed: 71                       6 non-null      object\n",
      " 72  Unnamed: 72                       6 non-null      object\n",
      " 73  Unnamed: 73                       6 non-null      object\n",
      " 74  Unnamed: 74                       6 non-null      object\n",
      " 75  Unnamed: 75                       6 non-null      object\n",
      " 76  Unnamed: 76                       6 non-null      object\n",
      " 77  Unnamed: 77                       2 non-null      object\n",
      " 78  Unnamed: 78                       2 non-null      object\n",
      " 79  Unnamed: 79                       2 non-null      object\n",
      " 80  Unnamed: 80                       2 non-null      object\n",
      " 81  Unnamed: 81                       2 non-null      object\n",
      " 82  Unnamed: 82                       2 non-null      object\n",
      " 83  Unnamed: 83                       2 non-null      object\n",
      " 84  Unnamed: 84                       2 non-null      object\n",
      " 85  Unnamed: 85                       2 non-null      object\n",
      " 86  Unnamed: 86                       2 non-null      object\n",
      " 87  Unnamed: 87                       2 non-null      object\n",
      "dtypes: object(88)\n",
      "memory usage: 32.4+ KB\n"
     ]
    }
   ],
   "source": [
    "sheet2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T05:41:56.597853Z",
     "iopub.status.busy": "2025-12-01T05:41:56.597437Z",
     "iopub.status.idle": "2025-12-01T05:41:56.608785Z",
     "shell.execute_reply": "2025-12-01T05:41:56.607662Z",
     "shell.execute_reply.started": "2025-12-01T05:41:56.597728Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 47 entries, 0 to 46\n",
      "Data columns (total 23 columns):\n",
      " #   Column                                   Non-Null Count  Dtype \n",
      "---  ------                                   --------------  ----- \n",
      " 0   Overview of Company Information          46 non-null     object\n",
      " 1   Unnamed: 1                               46 non-null     object\n",
      " 2   Unnamed: 2                               46 non-null     object\n",
      " 3   Unnamed: 3                               46 non-null     object\n",
      " 4   Unnamed: 4                               46 non-null     object\n",
      " 5   Unnamed: 5                               46 non-null     object\n",
      " 6   Unnamed: 6                               46 non-null     object\n",
      " 7   Unnamed: 7                               46 non-null     object\n",
      " 8   Compliance with Regulatory Requirements  47 non-null     object\n",
      " 9   Unnamed: 9                               46 non-null     object\n",
      " 10  Unnamed: 10                              42 non-null     object\n",
      " 11  Unnamed: 11                              37 non-null     object\n",
      " 12  Unnamed: 12                              47 non-null     object\n",
      " 13  Unnamed: 13                              46 non-null     object\n",
      " 14  Unnamed: 14                              38 non-null     object\n",
      " 15  Unnamed: 15                              30 non-null     object\n",
      " 16  Unnamed: 16                              47 non-null     object\n",
      " 17  Unnamed: 17                              46 non-null     object\n",
      " 18  Unnamed: 18                              33 non-null     object\n",
      " 19  Unnamed: 19                              21 non-null     object\n",
      " 20  Unnamed: 20                              47 non-null     object\n",
      " 21  Unnamed: 21                              46 non-null     object\n",
      " 22  Unnamed: 22                              22 non-null     object\n",
      "dtypes: object(23)\n",
      "memory usage: 8.6+ KB\n"
     ]
    }
   ],
   "source": [
    "sheet3.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Multi-Agent System**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T05:41:57.040649Z",
     "iopub.status.busy": "2025-12-01T05:41:57.040323Z",
     "iopub.status.idle": "2025-12-01T05:42:47.013461Z",
     "shell.execute_reply": "2025-12-01T05:42:47.012630Z",
     "shell.execute_reply.started": "2025-12-01T05:41:57.040625Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, Any, List, Optional, Set, Tuple\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "from google.adk.agents import LlmAgent\n",
    "from google.adk.tools import FunctionTool\n",
    "from google.adk.runners import InMemoryRunner\n",
    "from google.genai import types as genai_types\n",
    "\n",
    "try:\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    SKLEARN_AVAILABLE = True\n",
    "except Exception as e:\n",
    "    SKLEARN_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T05:49:47.703323Z",
     "iopub.status.busy": "2025-12-01T05:49:47.702930Z",
     "iopub.status.idle": "2025-12-01T05:49:47.796896Z",
     "shell.execute_reply": "2025-12-01T05:49:47.795921Z",
     "shell.execute_reply.started": "2025-12-01T05:49:47.703294Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:asyncio:Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x79f51a88f310>\n",
      "ERROR:asyncio:Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x79f51882e990>\n",
      "ERROR:asyncio:Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x79f51890d1d0>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Initializing Multi-Agent System...\n",
      "âœ… Gemini API Key Loaded from Kaggle Secrets\n",
      "âœ… Google ADK imported successfully\n",
      "âœ… SciPy available\n",
      "âœ… Scikit-learn available\n",
      "âœ… Visualization libraries available\n",
      "âœ… NLTK available for text mining\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ”§ Initializing Multi-Agent System...\")\n",
    "\n",
    "# Load API Key\n",
    "try:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "    print(\"âœ… Gemini API Key Loaded from Kaggle Secrets\")\n",
    "except Exception:\n",
    "    if not os.getenv(\"GOOGLE_API_KEY\"):\n",
    "        print(\"âš ï¸ WARNING: GOOGLE_API_KEY not found. Please set it manually.\")\n",
    "    else:\n",
    "        print(\"âœ… Using existing GOOGLE_API_KEY from environment\")\n",
    "\n",
    "# Google ADK\n",
    "try:\n",
    "    from google.adk.agents import LlmAgent\n",
    "    from google.adk.tools import FunctionTool\n",
    "    from google.adk.runners import InMemoryRunner\n",
    "    from google.genai import types as genai_types\n",
    "    print(\"âœ… Google ADK imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ ERROR: Google ADK not available: {e}\")\n",
    "    raise\n",
    "\n",
    "# Scientific Computing\n",
    "try:\n",
    "    from scipy.stats import pearsonr\n",
    "    from scipy import stats\n",
    "    SCIPY_AVAILABLE = True\n",
    "    print(\"âœ… SciPy available\")\n",
    "except Exception:\n",
    "    SCIPY_AVAILABLE = False\n",
    "    print(\"âš ï¸ SciPy not found - using pandas for correlations\")\n",
    "\n",
    "try:\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    SKLEARN_AVAILABLE = True\n",
    "    print(\"âœ… Scikit-learn available\")\n",
    "except Exception:\n",
    "    SKLEARN_AVAILABLE = False\n",
    "    print(\"âš ï¸ Scikit-learn not found - ML features limited\")\n",
    "\n",
    "# Visualization\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    PLOTTING_AVAILABLE = True\n",
    "    print(\"âœ… Visualization libraries available\")\n",
    "except Exception:\n",
    "    PLOTTING_AVAILABLE = False\n",
    "    print(\"âš ï¸ Matplotlib/Seaborn not found - visualizations disabled\")\n",
    "\n",
    "# NLP for Text Mining\n",
    "try:\n",
    "    import nltk\n",
    "    from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "    try:\n",
    "        nltk.data.find('vader_lexicon')\n",
    "    except LookupError:\n",
    "        nltk.download('vader_lexicon', quiet=True)\n",
    "    NLTK_AVAILABLE = True\n",
    "    print(\"âœ… NLTK available for text mining\")\n",
    "except Exception:\n",
    "    NLTK_AVAILABLE = False\n",
    "    print(\"âš ï¸ NLTK not found - sentiment analysis disabled\")\n",
    "\n",
    "# Configuration\n",
    "KTC_EXCEL_PATH = \"KTC-2025-ICT-benchmark-data.xlsx\"\n",
    "GLOBAL_STATE: Dict[str, Any] = {\n",
    "    \"initialized\": False,\n",
    "    \"load_timestamp\": None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T05:49:48.077320Z",
     "iopub.status.busy": "2025-12-01T05:49:48.076975Z",
     "iopub.status.idle": "2025-12-01T05:49:48.085077Z",
     "shell.execute_reply": "2025-12-01T05:49:48.084044Z",
     "shell.execute_reply.started": "2025-12-01T05:49:48.077299Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def json_safe(obj: Any) -> Any:\n",
    "    import numpy as _np\n",
    "    import pandas as _pd\n",
    "\n",
    "    # Dict\n",
    "    if isinstance(obj, dict):\n",
    "        return {json_safe(k): json_safe(v) for k, v in obj.items()}\n",
    "\n",
    "    # List / Tuple / Set\n",
    "    if isinstance(obj, (list, tuple, set)):\n",
    "        return [json_safe(x) for x in obj]\n",
    "\n",
    "    # Numpy scalars\n",
    "    if isinstance(obj, (_np.integer,)):\n",
    "        return int(obj)\n",
    "    if isinstance(obj, (_np.floating,)):\n",
    "        return float(obj)\n",
    "    if isinstance(obj, (_np.bool_,)):\n",
    "        return bool(obj)\n",
    "\n",
    "    # Pandas Timestamp\n",
    "    if isinstance(obj, _pd.Timestamp):\n",
    "        return obj.isoformat()\n",
    "\n",
    "    # Numpy array\n",
    "    if isinstance(obj, _np.ndarray):\n",
    "        return [json_safe(x) for x in obj.tolist()]\n",
    "\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. DATA AGENT TOOLS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T05:49:48.671300Z",
     "iopub.status.busy": "2025-12-01T05:49:48.670080Z",
     "iopub.status.idle": "2025-12-01T05:49:48.708140Z",
     "shell.execute_reply": "2025-12-01T05:49:48.707156Z",
     "shell.execute_reply.started": "2025-12-01T05:49:48.671260Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def prepare_ktc_data() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Loads Sheet 1 (Scoring data) with comprehensive error handling.\n",
    "    Returns summary statistics for downstream analysis.\n",
    "    \"\"\"\n",
    "    print(\"ðŸ“Š DATA AGENT: Loading KnowTheChain 2025 ICT Benchmark...\")\n",
    "    try:\n",
    "        df = pd.read_excel(KTC_EXCEL_PATH, sheet_name=\"1) Scoring\", header=2)\n",
    "        print(f\" âœ“ Loaded {len(df)} rows from Sheet 1\")\n",
    "    except FileNotFoundError:\n",
    "        return json_safe({\n",
    "            \"status\": \"error\",\n",
    "            \"message\": f\"File not found: {KTC_EXCEL_PATH}. Please ensure the dataset is in the working directory.\"\n",
    "        })\n",
    "    except Exception as e:\n",
    "        return json_safe({\"status\": \"error\", \"message\": f\"Loading error: {str(e)}\"})\n",
    "\n",
    "    # Standardize metadata columns\n",
    "    if len(df.columns) > 6:\n",
    "        rename_map = {\n",
    "            df.columns[0]: \"Company_ID\",\n",
    "            df.columns[1]: \"Company_Name\",\n",
    "            df.columns[2]: \"Year\",\n",
    "            df.columns[3]: \"Country\",\n",
    "            df.columns[4]: \"Region\",\n",
    "            df.columns[5]: \"Subindustry\",\n",
    "            df.columns[6]: \"Market_Cap\"\n",
    "        }\n",
    "        df = df.rename(columns=rename_map)\n",
    "\n",
    "    # Map theme columns to internal names\n",
    "    theme_mapping = {\n",
    "        \"Total benchmark score\": \"Total_Benchmark\",\n",
    "        \"Commitment & Governance\": \"Commitment_Governance\",\n",
    "        \"Traceability & Risk Assessment\": \"Traceability_Risk\",\n",
    "        \"Purchasing Practices\": \"Purchasing_Practices\",\n",
    "        \"Recruitment\": \"Recruitment\",\n",
    "        \"Worker Voice\": \"Enabling_Workers\",\n",
    "        \"Enabling Workers' Rights\": \"Enabling_Workers\",\n",
    "        \"Monitoring\": \"Monitoring\",\n",
    "        \"Remedy\": \"Remedy\"\n",
    "    }\n",
    "\n",
    "    score_cols = []\n",
    "    for orig_col, internal_name in theme_mapping.items():\n",
    "        if orig_col in df.columns:\n",
    "            df[internal_name] = pd.to_numeric(df[orig_col], errors='coerce')\n",
    "            score_cols.append(internal_name)\n",
    "\n",
    "    if \"Company_Name\" in df.columns:\n",
    "        df = df.dropna(subset=[\"Company_Name\"]).reset_index(drop=True)\n",
    "\n",
    "    GLOBAL_STATE[\"ktc_scoring\"] = df\n",
    "    GLOBAL_STATE[\"score_columns\"] = score_cols\n",
    "    GLOBAL_STATE[\"initialized\"] = True\n",
    "    GLOBAL_STATE[\"load_timestamp\"] = datetime.now().isoformat()\n",
    "\n",
    "    stats_summary = {}\n",
    "    for col in score_cols:\n",
    "        if col in df.columns:\n",
    "            stats_summary[col] = {\n",
    "                \"mean\": float(round(df[col].mean(), 2)),\n",
    "                \"median\": float(round(df[col].median(), 2)),\n",
    "                \"std\": float(round(df[col].std(), 2)),\n",
    "                \"min\": float(round(df[col].min(), 2)),\n",
    "                \"max\": float(round(df[col].max(), 2))\n",
    "            }\n",
    "\n",
    "    regional_stats = {}\n",
    "    if \"Region\" in df.columns and \"Total_Benchmark\" in df.columns:\n",
    "        agg_df = df.groupby(\"Region\")[\"Total_Benchmark\"].agg(['mean', 'count']).round(2)\n",
    "        regional_stats = {\n",
    "            index: {'mean': float(row['mean']), 'count': int(row['count'])}\n",
    "            for index, row in agg_df.iterrows()\n",
    "        }\n",
    "\n",
    "    return json_safe({\n",
    "        \"status\": \"success\",\n",
    "        \"n_companies\": len(df),\n",
    "        \"n_regions\": df[\"Region\"].nunique() if \"Region\" in df.columns else 0,\n",
    "        \"theme_statistics\": stats_summary,\n",
    "        \"regional_summary\": regional_stats,\n",
    "        \"score_columns_available\": score_cols,\n",
    "        \"data_quality\": {\n",
    "            \"missing_scores\": {col: int(df[col].isna().sum()) for col in score_cols if col in df.columns}\n",
    "        }\n",
    "    })\n",
    "\n",
    "\n",
    "def get_company_profile(company_name: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Retrieves detailed profile for a specific company.\n",
    "    Supports fuzzy matching.\n",
    "    \"\"\"\n",
    "    df = GLOBAL_STATE.get(\"ktc_scoring\")\n",
    "    if df is None:\n",
    "        prep_result = prepare_ktc_data()\n",
    "        if prep_result[\"status\"] == \"error\":\n",
    "            return json_safe(prep_result)\n",
    "        df = GLOBAL_STATE[\"ktc_scoring\"]\n",
    "\n",
    "    mask = df[\"Company_Name\"].str.contains(company_name, case=False, na=False, regex=False)\n",
    "    matches = df[mask]\n",
    "\n",
    "    if len(matches) == 0:\n",
    "        return json_safe({\n",
    "            \"status\": \"not_found\",\n",
    "            \"message\": f\"No company found matching '{company_name}'\",\n",
    "            \"suggestion\": \"Try searching with partial name or check spelling\"\n",
    "        })\n",
    "\n",
    "    if len(matches) > 1:\n",
    "        return json_safe({\n",
    "            \"status\": \"multiple_matches\",\n",
    "            \"count\": len(matches),\n",
    "            \"companies\": matches[\"Company_Name\"].tolist()[:5],\n",
    "            \"message\": \"Multiple companies found. Please be more specific.\"\n",
    "        })\n",
    "\n",
    "    row = matches.iloc[0]\n",
    "    score_cols = GLOBAL_STATE.get(\"score_columns\", [])\n",
    "\n",
    "    profile = {\n",
    "        \"status\": \"success\",\n",
    "        \"company\": row.get(\"Company_Name\"),\n",
    "        \"country\": row.get(\"Country\"),\n",
    "        \"region\": row.get(\"Region\"),\n",
    "        \"subindustry\": row.get(\"Subindustry\"),\n",
    "        \"scores\": {\n",
    "            col: float(row[col]) if pd.notna(row[col]) else None\n",
    "            for col in score_cols if col in row.index\n",
    "        }\n",
    "    }\n",
    "\n",
    "    if \"Total_Benchmark\" in df.columns:\n",
    "        df_sorted = df.sort_values(\"Total_Benchmark\", ascending=False).reset_index(drop=True)\n",
    "        rank = df_sorted[df_sorted[\"Company_Name\"] == row[\"Company_Name\"]].index[0] + 1\n",
    "        profile[\"overall_rank\"] = f\"{rank}/{len(df)}\"\n",
    "\n",
    "    return json_safe(profile)\n",
    "\n",
    "\n",
    "def get_leaderboard(sort_by: str = \"Total_Benchmark\",\n",
    "                    top_k: int = 10,\n",
    "                    region_filter: Optional[str] = None) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Returns top performers by specified metric.\n",
    "    Supports regional filtering.\n",
    "    \"\"\"\n",
    "    df = GLOBAL_STATE.get(\"ktc_scoring\")\n",
    "    if df is None:\n",
    "        prepare_ktc_data()\n",
    "        df = GLOBAL_STATE[\"ktc_scoring\"]\n",
    "\n",
    "    sort_col = None\n",
    "    for col in df.columns:\n",
    "        if sort_by.lower().replace(\" \", \"_\") in col.lower().replace(\" \", \"_\"):\n",
    "            sort_col = col\n",
    "            break\n",
    "\n",
    "    if sort_col is None:\n",
    "        sort_col = \"Total_Benchmark\"\n",
    "\n",
    "    df_filtered = df.copy()\n",
    "    if region_filter and \"Region\" in df.columns:\n",
    "        df_filtered = df_filtered[df_filtered[\"Region\"].str.contains(region_filter, case=False, na=False)]\n",
    "\n",
    "    top = df_filtered.sort_values(sort_col, ascending=False).head(top_k)\n",
    "\n",
    "    leaderboard = []\n",
    "    for _, row in top.iterrows():\n",
    "        leaderboard.append({\n",
    "            \"rank\": len(leaderboard) + 1,\n",
    "            \"company\": row.get(\"Company_Name\"),\n",
    "            \"country\": row.get(\"Country\"),\n",
    "            \"score\": float(row[sort_col]) if pd.notna(row[sort_col]) else None\n",
    "        })\n",
    "\n",
    "    return json_safe({\n",
    "        \"status\": \"success\",\n",
    "        \"criterion\": sort_col,\n",
    "        \"region_filter\": region_filter,\n",
    "        \"top_performers\": leaderboard,\n",
    "        \"average_of_top\": float(round(top[sort_col].mean(), 2)) if sort_col in top.columns else None\n",
    "    })\n",
    "\n",
    "\n",
    "def load_detailed_research() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Loads Sheet 2 (Detailed scoring & research) for indicator-level analysis.\n",
    "    \"\"\"\n",
    "    print(\"ðŸ“‹ DATA AGENT: Loading detailed indicator data...\")\n",
    "    try:\n",
    "        df = pd.read_excel(KTC_EXCEL_PATH, sheet_name=\"2) Detailed scoring & research\", header=2)\n",
    "    except Exception:\n",
    "        return json_safe({\"status\": \"error\", \"message\": \"Sheet 2 not accessible\"})\n",
    "\n",
    "    indicator_cols = [c for c in df.columns if re.match(r'^\\d+\\.\\d+', str(c))]\n",
    "\n",
    "    if \"Company\" not in df.columns and len(df.columns) > 0:\n",
    "        df.rename(columns={df.columns[0]: \"Company\"}, inplace=True)\n",
    "\n",
    "    df_long = df.melt(\n",
    "        id_vars=[\"Company\"],\n",
    "        value_vars=indicator_cols,\n",
    "        var_name=\"Indicator\",\n",
    "        value_name=\"Score\"\n",
    "    )\n",
    "\n",
    "    df_long[\"Theme\"] = df_long[\"Indicator\"].astype(str).str.split(\".\").str[0]\n",
    "    df_long[\"Score\"] = pd.to_numeric(df_long[\"Score\"], errors='coerce')\n",
    "\n",
    "    GLOBAL_STATE[\"detailed_df\"] = df_long\n",
    "    GLOBAL_STATE[\"detailed_raw\"] = df  # keep original for text mining if needed\n",
    "\n",
    "    return json_safe({\n",
    "        \"status\": \"success\",\n",
    "        \"n_indicators\": len(indicator_cols),\n",
    "        \"n_observations\": len(df_long),\n",
    "        \"themes_covered\": sorted(df_long[\"Theme\"].unique().tolist())\n",
    "    })\n",
    "\n",
    "\n",
    "def load_non_scored_research() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Loads Sheet 3 (Non-scored research) for text mining.\n",
    "    \"\"\"\n",
    "    print(\"ðŸ“„ DATA AGENT: Loading non-scored qualitative data...\")\n",
    "    try:\n",
    "        df = pd.read_excel(KTC_EXCEL_PATH, sheet_name=\"3) Non-scored research\", header=2)\n",
    "    except Exception:\n",
    "        return json_safe({\"status\": \"error\", \"message\": \"Sheet 3 not accessible\"})\n",
    "\n",
    "    text_cols = [c for c in df.columns if any(\n",
    "        keyword in str(c).lower()\n",
    "        for keyword in [\"comment\", \"summary\", \"allegation\", \"statement\", \"description\", \"note\", \"msa\"]\n",
    "    )]\n",
    "\n",
    "    if text_cols:\n",
    "        df[\"Combined_Text\"] = df[text_cols].fillna(\"\").astype(str).agg(\" \".join, axis=1)\n",
    "    else:\n",
    "        df[\"Combined_Text\"] = \"\"\n",
    "\n",
    "    if \"Company\" not in df.columns and len(df.columns) > 0:\n",
    "        df.rename(columns={df.columns[0]: \"Company\"}, inplace=True)\n",
    "\n",
    "    GLOBAL_STATE[\"non_scored_df\"] = df\n",
    "\n",
    "    return json_safe({\n",
    "        \"status\": \"success\",\n",
    "        \"n_companies\": len(df),\n",
    "        \"text_columns_found\": text_cols,\n",
    "        \"avg_text_length\": int(df[\"Combined_Text\"].str.len().mean()) if \"Combined_Text\" in df.columns else 0\n",
    "    })\n",
    "\n",
    "\n",
    "def filter_companies_by_region(region: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Filters companies in the Scoring sheet by Region (case-insensitive substring match)\n",
    "    and returns their Company_Name and Total_Benchmark scores.\n",
    "    \"\"\"\n",
    "    df = GLOBAL_STATE.get(\"ktc_scoring\")\n",
    "    if df is None:\n",
    "        res = prepare_ktc_data()\n",
    "        if res.get(\"status\") == \"error\":\n",
    "            return json_safe(res)\n",
    "        df = GLOBAL_STATE[\"ktc_scoring\"]\n",
    "\n",
    "    if \"Region\" not in df.columns or \"Total_Benchmark\" not in df.columns:\n",
    "        return json_safe({\"status\": \"error\", \"message\": \"Region or Total_Benchmark column missing\"})\n",
    "\n",
    "    mask = df[\"Region\"].astype(str).str.contains(region, case=False, na=False)\n",
    "    subset = df.loc[mask, [\"Company_Name\", \"Region\", \"Total_Benchmark\"]].copy()\n",
    "\n",
    "    companies = [\n",
    "        {\n",
    "            \"company\": row[\"Company_Name\"],\n",
    "            \"region\": row[\"Region\"],\n",
    "            \"total_benchmark\": float(row[\"Total_Benchmark\"]) if pd.notna(row[\"Total_Benchmark\"]) else None\n",
    "        }\n",
    "        for _, row in subset.iterrows()\n",
    "    ]\n",
    "\n",
    "    return json_safe({\n",
    "        \"status\": \"success\",\n",
    "        \"region_query\": region,\n",
    "        \"n_companies\": len(companies),\n",
    "        \"companies\": companies\n",
    "    })\n",
    "\n",
    "\n",
    "def clean_scores_and_top_companies(min_total: float = 0.0,\n",
    "                                   top_k: int = 5) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Cleans the Scoring sheet by removing rows with missing or <= min_total\n",
    "    Total_Benchmark scores, then returns the top_k companies by Total_Benchmark\n",
    "    descending.\n",
    "    \"\"\"\n",
    "    df = GLOBAL_STATE.get(\"ktc_scoring\")\n",
    "    if df is None:\n",
    "        res = prepare_ktc_data()\n",
    "        if res.get(\"status\") == \"error\":\n",
    "            return json_safe(res)\n",
    "        df = GLOBAL_STATE[\"ktc_scoring\"]\n",
    "\n",
    "    if \"Total_Benchmark\" not in df.columns:\n",
    "        return json_safe({\"status\": \"error\", \"message\": \"Total_Benchmark column not available\"})\n",
    "\n",
    "    cleaned = df.dropna(subset=[\"Total_Benchmark\"])\n",
    "    cleaned = cleaned[cleaned[\"Total_Benchmark\"] > min_total]\n",
    "\n",
    "    top = cleaned.sort_values(\"Total_Benchmark\", ascending=False).head(top_k)\n",
    "\n",
    "    results = [\n",
    "        {\n",
    "            \"rank\": idx + 1,\n",
    "            \"company\": row[\"Company_Name\"],\n",
    "            \"country\": row.get(\"Country\"),\n",
    "            \"total_benchmark\": float(row[\"Total_Benchmark\"])\n",
    "        }\n",
    "        for idx, (_, row) in enumerate(top.iterrows())\n",
    "    ]\n",
    "\n",
    "    return json_safe({\n",
    "        \"status\": \"success\",\n",
    "        \"min_total\": float(min_total),\n",
    "        \"top_k\": top_k,\n",
    "        \"n_after_cleaning\": len(cleaned),\n",
    "        \"top_companies\": results\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. ANALYSIS AGENT TOOLS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T05:49:49.162316Z",
     "iopub.status.busy": "2025-12-01T05:49:49.161955Z",
     "iopub.status.idle": "2025-12-01T05:49:49.201110Z",
     "shell.execute_reply": "2025-12-01T05:49:49.200027Z",
     "shell.execute_reply.started": "2025-12-01T05:49:49.162290Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_theme_correlations() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Computes Pearson correlations between themes using scipy.\n",
    "    \"\"\"\n",
    "    df = GLOBAL_STATE.get(\"ktc_scoring\")\n",
    "    if df is None:\n",
    "        prepare_ktc_data()\n",
    "        df = GLOBAL_STATE[\"ktc_scoring\"]\n",
    "\n",
    "    themes = [\n",
    "        \"Commitment_Governance\",\n",
    "        \"Traceability_Risk\",\n",
    "        \"Purchasing_Practices\",\n",
    "        \"Recruitment\",\n",
    "        \"Enabling_Workers\",\n",
    "        \"Monitoring\",\n",
    "        \"Remedy\"\n",
    "    ]\n",
    "    available_themes = [t for t in themes if t in df.columns]\n",
    "\n",
    "    if len(available_themes) < 2:\n",
    "        return json_safe({\"status\": \"error\", \"message\": \"Insufficient themes for correlation\"})\n",
    "\n",
    "    if SCIPY_AVAILABLE:\n",
    "        corr_matrix: Dict[str, Dict[str, Any]] = {}\n",
    "        for t1 in available_themes:\n",
    "            corr_matrix[t1] = {}\n",
    "            for t2 in available_themes:\n",
    "                data1 = df[t1].dropna()\n",
    "                data2 = df[t2].dropna()\n",
    "                common_idx = data1.index.intersection(data2.index)\n",
    "                if len(common_idx) > 2:\n",
    "                    corr, pval = pearsonr(df.loc[common_idx, t1], df.loc[common_idx, t2])\n",
    "                    corr_matrix[t1][t2] = {\n",
    "                        \"correlation\": float(round(corr, 2)),\n",
    "                        \"p_value\": float(round(pval, 4))\n",
    "                    }\n",
    "                else:\n",
    "                    corr_matrix[t1][t2] = {\"correlation\": None, \"p_value\": None}\n",
    "    else:\n",
    "        corr_df = df[available_themes].corr()\n",
    "        corr_matrix = corr_df.round(2).to_dict()\n",
    "\n",
    "    strong_correlations = []\n",
    "    for t1 in corr_matrix:\n",
    "        for t2 in corr_matrix[t1]:\n",
    "            if t1 < t2:\n",
    "                if isinstance(corr_matrix[t1][t2], dict):\n",
    "                    corr_val = corr_matrix[t1][t2].get(\"correlation\")\n",
    "                else:\n",
    "                    corr_val = corr_matrix[t1][t2]\n",
    "                if corr_val is not None and abs(corr_val) > 0.7:\n",
    "                    strong_correlations.append({\n",
    "                        \"theme1\": t1,\n",
    "                        \"theme2\": t2,\n",
    "                        \"correlation\": float(corr_val)\n",
    "                    })\n",
    "\n",
    "    GLOBAL_STATE[\"correlation_matrix\"] = corr_matrix\n",
    "\n",
    "    return json_safe({\n",
    "        \"status\": \"success\",\n",
    "        \"correlation_matrix\": corr_matrix,\n",
    "        \"strong_correlations\": sorted(\n",
    "            strong_correlations,\n",
    "            key=lambda x: abs(x[\"correlation\"]),\n",
    "            reverse=True\n",
    "        )[:5],\n",
    "        \"interpretation\": \"High correlation (>0.7) suggests themes are interconnected - improving one may help others\"\n",
    "    })\n",
    "\n",
    "\n",
    "def perform_clustering(n_clusters: int = 3, method: str = \"kmeans\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Clusters companies based on their theme scores using k-means.\n",
    "    \"\"\"\n",
    "    if not SKLEARN_AVAILABLE:\n",
    "        return json_safe({\"status\": \"error\", \"message\": \"scikit-learn required for clustering\"})\n",
    "\n",
    "    df = GLOBAL_STATE.get(\"ktc_scoring\")\n",
    "    if df is None:\n",
    "        prepare_ktc_data()\n",
    "        df = GLOBAL_STATE[\"ktc_scoring\"]\n",
    "\n",
    "    feature_cols = [\n",
    "        \"Commitment_Governance\", \"Traceability_Risk\", \"Purchasing_Practices\",\n",
    "        \"Recruitment\", \"Enabling_Workers\", \"Monitoring\", \"Remedy\"\n",
    "    ]\n",
    "    available_features = [f for f in feature_cols if f in df.columns]\n",
    "\n",
    "    if len(available_features) < 3:\n",
    "        return json_safe({\"status\": \"error\", \"message\": \"Insufficient features for clustering\"})\n",
    "\n",
    "    X = df[available_features].fillna(0).values\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n",
    "    df[\"Cluster\"] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "    GLOBAL_STATE[\"ktc_scoring\"] = df\n",
    "\n",
    "    cluster_profiles = {}\n",
    "    for cluster_id in range(n_clusters):\n",
    "        cluster_data = df[df[\"Cluster\"] == cluster_id]\n",
    "        profile = {\n",
    "            \"count\": int(len(cluster_data)),\n",
    "            \"percentage\": float(round(len(cluster_data) / len(df) * 100, 1)),\n",
    "            \"avg_total_score\": float(round(cluster_data[\"Total_Benchmark\"].mean(), 2))\n",
    "            if \"Total_Benchmark\" in cluster_data.columns else None,\n",
    "            \"theme_averages\": {\n",
    "                feat: float(round(cluster_data[feat].mean(), 2))\n",
    "                for feat in available_features\n",
    "            },\n",
    "            \"example_companies\": cluster_data[\"Company_Name\"].head(3).tolist()\n",
    "            if \"Company_Name\" in cluster_data.columns else []\n",
    "        }\n",
    "        cluster_profiles[f\"Cluster_{cluster_id}\"] = profile\n",
    "\n",
    "    return json_safe({\n",
    "        \"status\": \"success\",\n",
    "        \"n_clusters\": n_clusters,\n",
    "        \"cluster_profiles\": cluster_profiles,\n",
    "        \"interpretation\": \"Clusters group companies with similar ethical performance patterns\"\n",
    "    })\n",
    "\n",
    "\n",
    "def analyze_indicator_variance(top_k: int = 5) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Identifies indicators with highest variance (inconsistency across companies).\n",
    "    \"\"\"\n",
    "    if \"detailed_df\" not in GLOBAL_STATE:\n",
    "        load_detailed_research()\n",
    "\n",
    "    df = GLOBAL_STATE.get(\"detailed_df\")\n",
    "    if df is None:\n",
    "        return json_safe({\"status\": \"error\", \"message\": \"Detailed data not loaded\"})\n",
    "\n",
    "    indicator_stats = df.groupby(\"Indicator\")[\"Score\"].agg([\n",
    "        ('mean', 'mean'),\n",
    "        ('std', 'std'),\n",
    "        ('min', 'min'),\n",
    "        ('max', 'max'),\n",
    "        ('range', lambda x: x.max() - x.min())\n",
    "    ]).reset_index()\n",
    "\n",
    "    high_variance = indicator_stats.nlargest(top_k, 'std').to_dict(orient=\"records\")\n",
    "\n",
    "    theme_variance = {}\n",
    "    for theme in df[\"Theme\"].unique():\n",
    "        theme_indicators = indicator_stats[indicator_stats[\"Indicator\"].str.startswith(str(theme))]\n",
    "        if len(theme_indicators) > 0:\n",
    "            theme_variance[f\"Theme_{theme}\"] = {\n",
    "                \"avg_std\": float(round(theme_indicators[\"std\"].mean(), 2)),\n",
    "                \"most_variable\": theme_indicators.nlargest(2, 'std')[\"Indicator\"].tolist()\n",
    "            }\n",
    "\n",
    "    return json_safe({\n",
    "        \"status\": \"success\",\n",
    "        \"high_variance_indicators\": high_variance,\n",
    "        \"theme_variance_summary\": theme_variance,\n",
    "        \"interpretation\": \"High variance indicators show inconsistent implementation across industry\"\n",
    "    })\n",
    "\n",
    "\n",
    "def regional_comparison(regions: Optional[List[str]] = None) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Compares performance across regions.\n",
    "    \"\"\"\n",
    "    df = GLOBAL_STATE.get(\"ktc_scoring\")\n",
    "    if df is None or \"Region\" not in df.columns:\n",
    "        return json_safe({\"status\": \"error\", \"message\": \"Regional data not available\"})\n",
    "\n",
    "    if regions is None:\n",
    "        regions = df[\"Region\"].unique().tolist()\n",
    "\n",
    "    score_cols = GLOBAL_STATE.get(\"score_columns\", [])\n",
    "\n",
    "    regional_analysis = {}\n",
    "    for region in regions:\n",
    "        region_df = df[df[\"Region\"] == region]\n",
    "        if len(region_df) == 0:\n",
    "            continue\n",
    "        regional_analysis[region] = {\n",
    "            \"n_companies\": int(len(region_df)),\n",
    "            \"avg_scores\": {\n",
    "                col: float(round(region_df[col].mean(), 2))\n",
    "                for col in score_cols if col in region_df.columns\n",
    "            }\n",
    "        }\n",
    "\n",
    "    return json_safe({\n",
    "        \"status\": \"success\",\n",
    "        \"regional_comparison\": regional_analysis,\n",
    "        \"interpretation\": \"Regional differences may indicate varying regulatory environments and corporate cultures\"\n",
    "    })\n",
    "\n",
    "\n",
    "def total_benchmark_stats() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Computes the mean and standard deviation of Total_Benchmark across all companies.\n",
    "    \"\"\"\n",
    "    df = GLOBAL_STATE.get(\"ktc_scoring\")\n",
    "    if df is None:\n",
    "        res = prepare_ktc_data()\n",
    "        if res.get(\"status\") == \"error\":\n",
    "            return json_safe(res)\n",
    "        df = GLOBAL_STATE[\"ktc_scoring\"]\n",
    "\n",
    "    if \"Total_Benchmark\" not in df.columns:\n",
    "        return json_safe({\"status\": \"error\", \"message\": \"Total_Benchmark column not available\"})\n",
    "\n",
    "    mean_val = df[\"Total_Benchmark\"].mean()\n",
    "    std_val = df[\"Total_Benchmark\"].std()\n",
    "\n",
    "    return json_safe({\n",
    "        \"status\": \"success\",\n",
    "        \"mean\": float(round(mean_val, 2)),\n",
    "        \"std\": float(round(std_val, 2))\n",
    "    })\n",
    "\n",
    "\n",
    "def marketcap_totalbenchmark_correlation() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Calculates the correlation between Market_Cap and Total_Benchmark.\n",
    "    \"\"\"\n",
    "    df = GLOBAL_STATE.get(\"ktc_scoring\")\n",
    "    if df is None:\n",
    "        res = prepare_ktc_data()\n",
    "        if res.get(\"status\") == \"error\":\n",
    "            return json_safe(res)\n",
    "        df = GLOBAL_STATE[\"ktc_scoring\"]\n",
    "\n",
    "    if \"Market_Cap\" not in df.columns or \"Total_Benchmark\" not in df.columns:\n",
    "        return json_safe({\"status\": \"error\", \"message\": \"Market_Cap or Total_Benchmark not in dataset\"})\n",
    "\n",
    "    sub = df[[\"Market_Cap\", \"Total_Benchmark\"]].dropna()\n",
    "    if len(sub) < 3:\n",
    "        return json_safe({\"status\": \"error\", \"message\": \"Insufficient data for correlation\"})\n",
    "\n",
    "    if SCIPY_AVAILABLE:\n",
    "        corr, pval = pearsonr(sub[\"Market_Cap\"], sub[\"Total_Benchmark\"])\n",
    "        return json_safe({\n",
    "            \"status\": \"success\",\n",
    "            \"correlation\": float(round(corr, 3)),\n",
    "            \"p_value\": float(round(pval, 4))\n",
    "        })\n",
    "    else:\n",
    "        corr = sub.corr().loc[\"Market_Cap\", \"Total_Benchmark\"]\n",
    "        return json_safe({\n",
    "            \"status\": \"success\",\n",
    "            \"correlation\": float(round(corr, 3)),\n",
    "            \"note\": \"p_value not available without SciPy\"\n",
    "        })\n",
    "\n",
    "\n",
    "def custom_clustering(n_clusters: int = 3,\n",
    "                      feature_columns: Optional[List[str]] = None) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Performs k-means clustering on specified numeric columns.\n",
    "    \"\"\"\n",
    "    if not SKLEARN_AVAILABLE:\n",
    "        return json_safe({\"status\": \"error\", \"message\": \"scikit-learn required for clustering\"})\n",
    "\n",
    "    df = GLOBAL_STATE.get(\"ktc_scoring\")\n",
    "    if df is None:\n",
    "        res = prepare_ktc_data()\n",
    "        if res.get(\"status\") == \"error\":\n",
    "            return json_safe(res)\n",
    "        df = GLOBAL_STATE[\"ktc_scoring\"]\n",
    "\n",
    "    if feature_columns is None:\n",
    "        feature_columns = [\"Total_Benchmark\", \"Purchasing_Practices\"]\n",
    "\n",
    "    for col in feature_columns:\n",
    "        if col not in df.columns:\n",
    "            return json_safe({\"status\": \"error\", \"message\": f\"Column '{col}' not in dataset\"})\n",
    "\n",
    "    X = df[feature_columns].fillna(0).values\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n",
    "    labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "    df[\"Custom_Cluster\"] = labels\n",
    "    GLOBAL_STATE[\"ktc_scoring\"] = df\n",
    "\n",
    "    clusters = {}\n",
    "    for cid in range(n_clusters):\n",
    "        subset = df[df[\"Custom_Cluster\"] == cid]\n",
    "        clusters[f\"Cluster_{cid}\"] = {\n",
    "            \"n_companies\": int(len(subset)),\n",
    "            \"companies\": subset[\"Company_Name\"].tolist(),\n",
    "            \"centroid_features\": {\n",
    "                feature_columns[i]: float(round(val, 2))\n",
    "                for i, val in enumerate(kmeans.cluster_centers_[cid])\n",
    "            }\n",
    "        }\n",
    "\n",
    "    return json_safe({\n",
    "        \"status\": \"success\",\n",
    "        \"features_used\": feature_columns,\n",
    "        \"clusters\": clusters\n",
    "    })\n",
    "\n",
    "\n",
    "def median_theme_scores_by_region(region: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Computes the median score for each theme for companies in the given region.\n",
    "    \"\"\"\n",
    "    df = GLOBAL_STATE.get(\"ktc_scoring\")\n",
    "    if df is None:\n",
    "        res = prepare_ktc_data()\n",
    "        if res.get(\"status\") == \"error\":\n",
    "            return json_safe(res)\n",
    "        df = GLOBAL_STATE[\"ktc_scoring\"]\n",
    "\n",
    "    if \"Region\" not in df.columns:\n",
    "        return json_safe({\"status\": \"error\", \"message\": \"Region column not available\"})\n",
    "\n",
    "    score_cols = GLOBAL_STATE.get(\"score_columns\", [])\n",
    "    mask = df[\"Region\"].astype(str).str.contains(region, case=False, na=False)\n",
    "    subset = df.loc[mask, score_cols]\n",
    "\n",
    "    if subset.empty:\n",
    "        return json_safe({\"status\": \"error\", \"message\": f\"No companies found for region '{region}'\"})\n",
    "\n",
    "    medians = {col: float(round(subset[col].median(), 2)) for col in score_cols}\n",
    "\n",
    "    return json_safe({\n",
    "        \"status\": \"success\",\n",
    "        \"region\": region,\n",
    "        \"n_companies\": int(mask.sum()),\n",
    "        \"median_scores\": medians\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. RESEARCH AGENT TOOLS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T05:49:49.494671Z",
     "iopub.status.busy": "2025-12-01T05:49:49.494348Z",
     "iopub.status.idle": "2025-12-01T05:49:49.506986Z",
     "shell.execute_reply": "2025-12-01T05:49:49.505970Z",
     "shell.execute_reply.started": "2025-12-01T05:49:49.494647Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def search_external_validation(query: str,\n",
    "                               source_preference: str = \"ILO\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Simulates external research validation (placeholder).\n",
    "    \"\"\"\n",
    "    print(f\"ðŸ” RESEARCH AGENT: Searching for '{query}' from {source_preference}...\")\n",
    "\n",
    "    knowledge_base = {\n",
    "        \"forced labour statistics\": {\n",
    "            \"source\": \"ILO 2021 Global Estimates\",\n",
    "            \"finding\": \"27.6 million victims of forced labour globally\",\n",
    "            \"relevance\": \"Benchmark figure for ICT sector context\",\n",
    "            \"url\": \"https://www.ilo.org/topics/forced-labour\"\n",
    "        },\n",
    "        \"taiwan migrant workers\": {\n",
    "            \"source\": \"Business & Human Rights Resource Centre 2025\",\n",
    "            \"finding\": \"59% of migrant workers in Taiwan ICT factories pay placement fees leading to debt bondage\",\n",
    "            \"relevance\": \"Regional risk factor\",\n",
    "            \"url\": \"https://www.business-humanrights.org/\"\n",
    "        },\n",
    "        \"semiconductor risks\": {\n",
    "            \"source\": \"KnowTheChain 2025 ICT Benchmark\",\n",
    "            \"finding\": \"80% of assessed companies linked to 42 unresolved forced labour allegations\",\n",
    "            \"relevance\": \"Industry-wide systemic issue\",\n",
    "            \"url\": \"https://knowthechain.org/\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    result = None\n",
    "    for key, data in knowledge_base.items():\n",
    "        if any(word in query.lower() for word in key.split()):\n",
    "            result = data\n",
    "            break\n",
    "\n",
    "    if result is None:\n",
    "        return json_safe({\n",
    "            \"status\": \"no_match\",\n",
    "            \"message\": f\"No cached data for '{query}'. In production, would search: {source_preference}\",\n",
    "            \"suggestion\": \"Try: 'forced labour statistics', 'taiwan migrant workers', or 'semiconductor risks'\"\n",
    "        })\n",
    "\n",
    "    return json_safe({\n",
    "        \"status\": \"success\",\n",
    "        \"query\": query,\n",
    "        \"external_data\": result\n",
    "    })\n",
    "\n",
    "\n",
    "def cross_reference_allegations(company_name: Optional[str] = None) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Cross-references companies with allegation-related text in Sheet 3.\n",
    "    \"\"\"\n",
    "    df = GLOBAL_STATE.get(\"non_scored_df\")\n",
    "    if df is None:\n",
    "        load_non_scored_research()\n",
    "        df = GLOBAL_STATE.get(\"non_scored_df\")\n",
    "\n",
    "    if df is None:\n",
    "        return json_safe({\"status\": \"error\", \"message\": \"Non-scored data not available\"})\n",
    "\n",
    "    allegation_keywords = [\"allegation\", \"violation\", \"forced labour\", \"debt bondage\", \"exploit\"]\n",
    "\n",
    "    if company_name:\n",
    "        company_df = df[df[\"Company\"].str.contains(company_name, case=False, na=False, regex=False)]\n",
    "        if len(company_df) == 0:\n",
    "            return json_safe({\"status\": \"not_found\", \"message\": f\"No data for {company_name}\"})\n",
    "        search_df = company_df\n",
    "    else:\n",
    "        search_df = df\n",
    "\n",
    "    allegation_count = 0\n",
    "    companies_with_allegations = []\n",
    "\n",
    "    for _, row in search_df.iterrows():\n",
    "        text = str(row.get(\"Combined_Text\", \"\")).lower()\n",
    "        if any(keyword in text for keyword in allegation_keywords):\n",
    "            allegation_count += 1\n",
    "            companies_with_allegations.append(row.get(\"Company\", \"Unknown\"))\n",
    "\n",
    "    return json_safe({\n",
    "        \"status\": \"success\",\n",
    "        \"total_checked\": int(len(search_df)),\n",
    "        \"companies_with_allegations\": int(len(set(companies_with_allegations))),\n",
    "        \"allegation_mentions\": int(allegation_count),\n",
    "        \"sample_companies\": list(set(companies_with_allegations))[:10]\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. PREDICTION AGENT TOOLS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T05:49:49.959517Z",
     "iopub.status.busy": "2025-12-01T05:49:49.959155Z",
     "iopub.status.idle": "2025-12-01T05:49:49.995765Z",
     "shell.execute_reply": "2025-12-01T05:49:49.994716Z",
     "shell.execute_reply.started": "2025-12-01T05:49:49.959490Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_regression_model(target: str = \"Total_Benchmark\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Trains regression model to identify key drivers of ethical performance.\n",
    "    \"\"\"\n",
    "    if not SKLEARN_AVAILABLE:\n",
    "        return json_safe({\"status\": \"error\", \"message\": \"scikit-learn required\"})\n",
    "\n",
    "    df = GLOBAL_STATE.get(\"ktc_scoring\")\n",
    "    if df is None:\n",
    "        prepare_ktc_data()\n",
    "        df = GLOBAL_STATE[\"ktc_scoring\"]\n",
    "\n",
    "    feature_cols = [\n",
    "        \"Commitment_Governance\", \"Traceability_Risk\", \"Purchasing_Practices\",\n",
    "        \"Recruitment\", \"Enabling_Workers\", \"Monitoring\", \"Remedy\"\n",
    "    ]\n",
    "    available_features = [f for f in feature_cols if f in df.columns]\n",
    "\n",
    "    if target not in df.columns:\n",
    "        return json_safe({\"status\": \"error\", \"message\": f\"Target '{target}' not found\"})\n",
    "\n",
    "    df_clean = df.dropna(subset=[target] + available_features)\n",
    "\n",
    "    if len(df_clean) < 10:\n",
    "        return json_safe({\"status\": \"error\", \"message\": \"Insufficient data for modeling\"})\n",
    "\n",
    "    X = df_clean[available_features]\n",
    "    y = df_clean[target]\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "\n",
    "    r2_score = model.score(X, y)\n",
    "    coefficients = dict(zip(available_features, model.coef_))\n",
    "    sorted_coeffs = dict(sorted(coefficients.items(), key=lambda x: abs(x[1]), reverse=True))\n",
    "\n",
    "    GLOBAL_STATE[\"regression_model\"] = model\n",
    "    GLOBAL_STATE[\"model_features\"] = available_features\n",
    "    GLOBAL_STATE[\"model_coefficients\"] = {k: float(v) for k, v in coefficients.items()}\n",
    "    GLOBAL_STATE[\"model_intercept\"] = float(model.intercept_)\n",
    "\n",
    "    top_driver = max(coefficients.items(), key=lambda x: abs(x[1]))\n",
    "\n",
    "    return json_safe({\n",
    "        \"status\": \"success\",\n",
    "        \"model_performance\": {\n",
    "            \"r2_score\": float(round(r2_score, 3)),\n",
    "            \"interpretation\": f\"Model explains {float(round(r2_score * 100, 1))}% of variance in {target}\"\n",
    "        },\n",
    "        \"feature_importance\": {k: float(v) for k, v in sorted_coeffs.items()},\n",
    "        \"top_driver\": {\n",
    "            \"feature\": top_driver[0],\n",
    "            \"coefficient\": float(round(top_driver[1], 3)),\n",
    "            \"interpretation\": f\"1-point increase in {top_driver[0]} â†’ {float(round(top_driver[1], 2))} point change in {target}\"\n",
    "        },\n",
    "        \"intercept\": float(round(model.intercept_, 2))\n",
    "    })\n",
    "\n",
    "\n",
    "def forecast_scenario(theme: str, percentage_increase: float) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Simulates impact of improving specific theme on Total_Benchmark.\n",
    "    \"\"\"\n",
    "    coefficients = GLOBAL_STATE.get(\"model_coefficients\")\n",
    "\n",
    "    if coefficients is None:\n",
    "        train_result = train_regression_model()\n",
    "        if train_result[\"status\"] == \"error\":\n",
    "            return json_safe(train_result)\n",
    "        coefficients = GLOBAL_STATE[\"model_coefficients\"]\n",
    "\n",
    "    matched_theme = None\n",
    "    for key in coefficients.keys():\n",
    "        if theme.lower() in key.lower():\n",
    "            matched_theme = key\n",
    "            break\n",
    "\n",
    "    if matched_theme is None:\n",
    "        return json_safe({\n",
    "            \"status\": \"not_found\",\n",
    "            \"message\": f\"Theme '{theme}' not found in model\",\n",
    "            \"available_themes\": list(coefficients.keys())\n",
    "        })\n",
    "\n",
    "    df = GLOBAL_STATE.get(\"ktc_scoring\")\n",
    "    current_avg = df[matched_theme].mean()\n",
    "\n",
    "    absolute_increase = current_avg * (percentage_increase / 100.0)\n",
    "    predicted_impact = absolute_increase * coefficients[matched_theme]\n",
    "\n",
    "    return json_safe({\n",
    "        \"status\": \"success\",\n",
    "        \"scenario\": {\n",
    "            \"theme\": matched_theme,\n",
    "            \"current_average\": float(round(current_avg, 2)),\n",
    "            \"percentage_increase\": float(percentage_increase),\n",
    "            \"absolute_increase\": float(round(absolute_increase, 2)),\n",
    "            \"predicted_impact_on_total\": float(round(predicted_impact, 2))\n",
    "        },\n",
    "        \"interpretation\": (\n",
    "            f\"If {matched_theme} improves by {percentage_increase}%, expect Total_Benchmark \"\n",
    "            f\"to increase by ~{float(round(predicted_impact, 2))} points\"\n",
    "        ),\n",
    "        \"coefficient\": float(round(coefficients[matched_theme], 3))\n",
    "    })\n",
    "\n",
    "\n",
    "def predict_score_improvements(target_percentile: float = 75.0) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Pathway recommendations to reach a target percentile.\n",
    "    \"\"\"\n",
    "    df = GLOBAL_STATE.get(\"ktc_scoring\")\n",
    "    coefficients = GLOBAL_STATE.get(\"model_coefficients\")\n",
    "\n",
    "    if df is None or coefficients is None:\n",
    "        return json_safe({\"status\": \"error\", \"message\": \"Model not trained or data not loaded\"})\n",
    "\n",
    "    if \"Total_Benchmark\" not in df.columns:\n",
    "        return json_safe({\"status\": \"error\", \"message\": \"Total_Benchmark not available\"})\n",
    "\n",
    "    target_score = df[\"Total_Benchmark\"].quantile(target_percentile / 100)\n",
    "    current_avg = df[\"Total_Benchmark\"].mean()\n",
    "    gap = target_score - current_avg\n",
    "\n",
    "    improvements = {}\n",
    "    for theme, coef in coefficients.items():\n",
    "        if abs(coef) > 0.01:\n",
    "            required_improvement = gap / coef\n",
    "            current_theme_avg = df[theme].mean()\n",
    "            percentage_needed = (\n",
    "                required_improvement / current_theme_avg * 100\n",
    "                if current_theme_avg > 0 else float('inf')\n",
    "            )\n",
    "\n",
    "            improvements[theme] = {\n",
    "                \"current_avg\": float(round(current_theme_avg, 2)),\n",
    "                \"required_improvement\": float(round(required_improvement, 2)),\n",
    "                \"percentage_increase_needed\": float(round(percentage_needed, 1))\n",
    "            }\n",
    "\n",
    "    return json_safe({\n",
    "        \"status\": \"success\",\n",
    "        \"target_percentile\": float(target_percentile),\n",
    "        \"target_score\": float(round(target_score, 2)),\n",
    "        \"current_average\": float(round(current_avg, 2)),\n",
    "        \"gap_to_close\": float(round(gap, 2)),\n",
    "        \"pathway_recommendations\": improvements,\n",
    "        \"interpretation\": (\n",
    "            f\"To reach {target_percentile}th percentile, companies need to \"\n",
    "            f\"improve by {float(round(gap, 2))} points\"\n",
    "        )\n",
    "    })\n",
    "\n",
    "\n",
    "def project_industry_average(years: int = 2,\n",
    "                             annual_growth_rate: float = 0.05) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Projects the industry average Total_Benchmark after `years` of\n",
    "    compound annual growth at `annual_growth_rate`.\n",
    "    \"\"\"\n",
    "    df = GLOBAL_STATE.get(\"ktc_scoring\")\n",
    "    if df is None:\n",
    "        res = prepare_ktc_data()\n",
    "        if res.get(\"status\") == \"error\":\n",
    "            return json_safe(res)\n",
    "        df = GLOBAL_STATE[\"ktc_scoring\"]\n",
    "\n",
    "    if \"Total_Benchmark\" not in df.columns:\n",
    "        return json_safe({\"status\": \"error\", \"message\": \"Total_Benchmark not available\"})\n",
    "\n",
    "    current_avg = df[\"Total_Benchmark\"].mean()\n",
    "    projected = current_avg * ((1 + annual_growth_rate) ** years)\n",
    "\n",
    "    return json_safe({\n",
    "        \"status\": \"success\",\n",
    "        \"current_average\": float(round(current_avg, 2)),\n",
    "        \"years\": int(years),\n",
    "        \"annual_growth_rate\": float(annual_growth_rate),\n",
    "        \"projected_average\": float(round(projected, 2))\n",
    "    })\n",
    "\n",
    "\n",
    "def predict_company_rank_change(company_name: str,\n",
    "                                theme: str = \"Remedy\",\n",
    "                                delta_points: float = 10.0) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Uses the regression model to estimate how a company's rank would change\n",
    "    if a specific theme (e.g., Remedy) increased by `delta_points`.\n",
    "    \"\"\"\n",
    "    df = GLOBAL_STATE.get(\"ktc_scoring\")\n",
    "    if df is None:\n",
    "        res = prepare_ktc_data()\n",
    "        if res.get(\"status\") == \"error\":\n",
    "            return json_safe(res)\n",
    "        df = GLOBAL_STATE[\"ktc_scoring\"]\n",
    "\n",
    "    model = GLOBAL_STATE.get(\"regression_model\")\n",
    "    features = GLOBAL_STATE.get(\"model_features\")\n",
    "\n",
    "    if model is None or features is None:\n",
    "        train_res = train_regression_model()\n",
    "        if train_res.get(\"status\") == \"error\":\n",
    "            return json_safe(train_res)\n",
    "        model = GLOBAL_STATE[\"regression_model\"]\n",
    "        features = GLOBAL_STATE[\"model_features\"]\n",
    "\n",
    "    mask = df[\"Company_Name\"].str.contains(company_name, case=False, na=False, regex=False)\n",
    "    if not mask.any():\n",
    "        return json_safe({\"status\": \"error\", \"message\": f\"Company '{company_name}' not found\"})\n",
    "\n",
    "    row = df.loc[mask].iloc[0]\n",
    "    if \"Total_Benchmark\" not in df.columns:\n",
    "        return json_safe({\"status\": \"error\", \"message\": \"Total_Benchmark not in dataset\"})\n",
    "\n",
    "    theme_col = None\n",
    "    for col in features:\n",
    "        if theme.lower() in col.lower():\n",
    "            theme_col = col\n",
    "            break\n",
    "    if theme_col is None:\n",
    "        return json_safe({\"status\": \"error\", \"message\": f\"Theme '{theme}' not in model features\"})\n",
    "\n",
    "    x = row[features].copy()\n",
    "    baseline_total = row[\"Total_Benchmark\"]\n",
    "\n",
    "    x_scenario = x.copy()\n",
    "    x_scenario[theme_col] = x_scenario[theme_col] + delta_points\n",
    "\n",
    "    baseline_pred = float(model.predict([x])[0])\n",
    "    scenario_pred = float(model.predict([x_scenario])[0])\n",
    "\n",
    "    all_totals = df[\"Total_Benchmark\"].tolist()\n",
    "    baseline_rank = 1 + sum(t > baseline_total for t in all_totals)\n",
    "    scenario_rank = 1 + sum(t > scenario_pred for t in all_totals)\n",
    "\n",
    "    return json_safe({\n",
    "        \"status\": \"success\",\n",
    "        \"company\": row[\"Company_Name\"],\n",
    "        \"theme\": theme_col,\n",
    "        \"delta_points\": float(delta_points),\n",
    "        \"baseline_total\": float(round(baseline_total, 2)),\n",
    "        \"baseline_predicted_total\": float(round(baseline_pred, 2)),\n",
    "        \"scenario_predicted_total\": float(round(scenario_pred, 2)),\n",
    "        \"baseline_rank\": int(baseline_rank),\n",
    "        \"scenario_rank\": int(scenario_rank),\n",
    "        \"rank_change\": int(baseline_rank - scenario_rank)\n",
    "    })\n",
    "\n",
    "\n",
    "def regional_theme_improvement(source_region: str,\n",
    "                               theme: str,\n",
    "                               target_value: float) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    For companies in `source_region`, simulate increasing the given theme\n",
    "    up to `target_value` and estimate average improvement in Total_Benchmark.\n",
    "    \"\"\"\n",
    "    df = GLOBAL_STATE.get(\"ktc_scoring\")\n",
    "    if df is None:\n",
    "        res = prepare_ktc_data()\n",
    "        if res.get(\"status\") == \"error\":\n",
    "            return json_safe(res)\n",
    "        df = GLOBAL_STATE[\"ktc_scoring\"]\n",
    "\n",
    "    model = GLOBAL_STATE.get(\"regression_model\")\n",
    "    features = GLOBAL_STATE.get(\"model_features\")\n",
    "\n",
    "    if model is None or features is None:\n",
    "        train_res = train_regression_model()\n",
    "        if train_res.get(\"status\") == \"error\":\n",
    "            return json_safe(train_res)\n",
    "        model = GLOBAL_STATE[\"regression_model\"]\n",
    "        features = GLOBAL_STATE[\"model_features\"]\n",
    "\n",
    "    if \"Region\" not in df.columns:\n",
    "        return json_safe({\"status\": \"error\", \"message\": \"Region column not available\"})\n",
    "    if \"Total_Benchmark\" not in df.columns:\n",
    "        return json_safe({\"status\": \"error\", \"message\": \"Total_Benchmark not available\"})\n",
    "\n",
    "    theme_col = None\n",
    "    for col in features:\n",
    "        if theme.lower() in col.lower():\n",
    "            theme_col = col\n",
    "            break\n",
    "    if theme_col is None:\n",
    "        return json_safe({\"status\": \"error\", \"message\": f\"Theme '{theme}' not in model features\"})\n",
    "\n",
    "    mask = df[\"Region\"].astype(str).str.contains(source_region, case=False, na=False)\n",
    "    region_df = df.loc[mask].copy()\n",
    "    if region_df.empty:\n",
    "        return json_safe({\"status\": \"error\", \"message\": f\"No companies found for region '{source_region}'\"})\n",
    "\n",
    "    improvements: List[float] = []\n",
    "    for _, row in region_df.iterrows():\n",
    "        x = row[features].copy()\n",
    "\n",
    "        if x[theme_col] >= target_value:\n",
    "            improvements.append(0.0)\n",
    "            continue\n",
    "\n",
    "        x_scenario = x.copy()\n",
    "        x_scenario[theme_col] = target_value\n",
    "\n",
    "        baseline_pred = float(model.predict([x])[0])\n",
    "        scenario_pred = float(model.predict([x_scenario])[0])\n",
    "        improvements.append(scenario_pred - baseline_pred)\n",
    "\n",
    "    avg_improvement = float(round(np.mean(improvements), 2))\n",
    "\n",
    "    return json_safe({\n",
    "        \"status\": \"success\",\n",
    "        \"region\": source_region,\n",
    "        \"theme\": theme_col,\n",
    "        \"target_value\": float(target_value),\n",
    "        \"avg_predicted_improvement\": avg_improvement,\n",
    "        \"n_companies\": int(len(improvements))\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5. TEXT MINING AGENT TOOLS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T05:49:50.216690Z",
     "iopub.status.busy": "2025-12-01T05:49:50.216317Z",
     "iopub.status.idle": "2025-12-01T05:49:50.239554Z",
     "shell.execute_reply": "2025-12-01T05:49:50.238607Z",
     "shell.execute_reply.started": "2025-12-01T05:49:50.216661Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def extract_text_sections(company_name: str,\n",
    "                          section_keywords: List[str]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extracts specific sections from non-scored research text.\n",
    "    \"\"\"\n",
    "    df = GLOBAL_STATE.get(\"non_scored_df\")\n",
    "    if df is None:\n",
    "        load_non_scored_research()\n",
    "        df = GLOBAL_STATE.get(\"non_scored_df\")\n",
    "\n",
    "    if df is None:\n",
    "        return json_safe({\"status\": \"error\", \"message\": \"Non-scored data not available\"})\n",
    "\n",
    "    company_mask = df[\"Company\"].str.contains(company_name, case=False, na=False, regex=False)\n",
    "    company_data = df[company_mask]\n",
    "\n",
    "    if len(company_data) == 0:\n",
    "        return json_safe({\"status\": \"not_found\", \"message\": f\"No text data for '{company_name}'\"})\n",
    "\n",
    "    text = str(company_data.iloc[0].get(\"Combined_Text\", \"\"))\n",
    "\n",
    "    extracted_sections = {}\n",
    "    for keyword in section_keywords:\n",
    "        pattern = rf\"(?i)({keyword}[^\\n]*(?:\\n(?!\\n)[^\\n]*)*)\"\n",
    "        matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "        if matches:\n",
    "            extracted_sections[keyword] = matches[:3]\n",
    "\n",
    "    return json_safe({\n",
    "        \"status\": \"success\",\n",
    "        \"company\": company_name,\n",
    "        \"sections_found\": list(extracted_sections.keys()),\n",
    "        \"extracted_content\": extracted_sections\n",
    "    })\n",
    "\n",
    "\n",
    "def perform_sentiment_analysis(company_name: Optional[str] = None) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Performs sentiment analysis on non-scored text data.\n",
    "    \"\"\"\n",
    "    if not NLTK_AVAILABLE:\n",
    "        return json_safe({\n",
    "            \"status\": \"limited\",\n",
    "            \"message\": \"NLTK not available - providing basic keyword analysis instead\"\n",
    "        })\n",
    "\n",
    "    df = GLOBAL_STATE.get(\"non_scored_df\")\n",
    "    if df is None:\n",
    "        load_non_scored_research()\n",
    "        df = GLOBAL_STATE.get(\"non_scored_df\")\n",
    "\n",
    "    if df is None:\n",
    "        return json_safe({\"status\": \"error\", \"message\": \"No text data available\"})\n",
    "\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "    if company_name:\n",
    "        df = df[df[\"Company\"].str.contains(company_name, case=False, na=False, regex=False)]\n",
    "        if len(df) == 0:\n",
    "            return json_safe({\"status\": \"not_found\", \"message\": f\"No data for {company_name}\"})\n",
    "\n",
    "    results = []\n",
    "    for _, row in df.iterrows():\n",
    "        text = str(row.get(\"Combined_Text\", \"\"))\n",
    "        if len(text) > 50:\n",
    "            scores = sia.polarity_scores(text)\n",
    "            results.append({\n",
    "                \"company\": row.get(\"Company\"),\n",
    "                \"sentiment\": {k: float(v) for k, v in scores.items()},\n",
    "                \"classification\": (\n",
    "                    \"positive\" if scores['compound'] > 0.05 else\n",
    "                    \"negative\" if scores['compound'] < -0.05 else\n",
    "                    \"neutral\"\n",
    "                )\n",
    "            })\n",
    "\n",
    "    if results:\n",
    "        avg_compound = sum(r[\"sentiment\"][\"compound\"] for r in results) / len(results)\n",
    "        sentiment_dist = {\n",
    "            \"positive\": sum(1 for r in results if r[\"classification\"] == \"positive\"),\n",
    "            \"neutral\": sum(1 for r in results if r[\"classification\"] == \"neutral\"),\n",
    "            \"negative\": sum(1 for r in results if r[\"classification\"] == \"negative\")\n",
    "        }\n",
    "    else:\n",
    "        avg_compound = 0.0\n",
    "        sentiment_dist = {}\n",
    "\n",
    "    return json_safe({\n",
    "        \"status\": \"success\",\n",
    "        \"n_analyzed\": int(len(results)),\n",
    "        \"overall_sentiment\": float(round(avg_compound, 3)),\n",
    "        \"distribution\": sentiment_dist,\n",
    "        \"details\": results[:5],\n",
    "        \"interpretation\": \"Positive sentiment suggests better disclosure quality\"\n",
    "    })\n",
    "\n",
    "\n",
    "def keyword_frequency_analysis(keywords: List[str],\n",
    "                               by_company: bool = False) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Analyzes keyword frequency across text data.\n",
    "    \"\"\"\n",
    "    df = GLOBAL_STATE.get(\"non_scored_df\")\n",
    "    if df is None:\n",
    "        load_non_scored_research()\n",
    "        df = GLOBAL_STATE.get(\"non_scored_df\")\n",
    "\n",
    "    if df is None:\n",
    "        return json_safe({\"status\": \"error\", \"message\": \"Text data not loaded\"})\n",
    "\n",
    "    keyword_counts = {kw: 0 for kw in keywords}\n",
    "    company_keyword_map: Dict[str, Dict[str, int]] = {}\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        text = str(row.get(\"Combined_Text\", \"\")).lower()\n",
    "        company = row.get(\"Company\", \"Unknown\")\n",
    "\n",
    "        if by_company and company not in company_keyword_map:\n",
    "            company_keyword_map[company] = {kw: 0 for kw in keywords}\n",
    "\n",
    "        for keyword in keywords:\n",
    "            count = text.count(keyword.lower())\n",
    "            keyword_counts[keyword] += count\n",
    "            if by_company:\n",
    "                company_keyword_map[company][keyword] += count\n",
    "\n",
    "    result: Dict[str, Any] = {\n",
    "        \"status\": \"success\",\n",
    "        \"keyword_frequencies\": keyword_counts,\n",
    "        \"total_documents\": int(len(df))\n",
    "    }\n",
    "\n",
    "    if by_company:\n",
    "        result[\"by_company\"] = company_keyword_map\n",
    "\n",
    "    return json_safe(result)\n",
    "\n",
    "\n",
    "def text_mining_summary(focus_areas: Optional[List[str]] = None) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Comprehensive text mining report combining keyword and sentiment analysis.\n",
    "    \"\"\"\n",
    "    if focus_areas is None:\n",
    "        focus_areas = [\n",
    "            \"forced labour\", \"migrant worker\", \"recruitment fee\",\n",
    "            \"debt bondage\", \"remedy\", \"grievance\", \"uk msa\"\n",
    "        ]\n",
    "\n",
    "    keyword_analysis = keyword_frequency_analysis(focus_areas)\n",
    "    sentiment_result = perform_sentiment_analysis()\n",
    "\n",
    "    return json_safe({\n",
    "        \"status\": \"success\",\n",
    "        \"focus_areas\": focus_areas,\n",
    "        \"keyword_analysis\": keyword_analysis.get(\"keyword_frequencies\", {}),\n",
    "        \"sentiment_overview\": {\n",
    "            \"avg_sentiment\": sentiment_result.get(\"overall_sentiment\", 0.0),\n",
    "            \"distribution\": sentiment_result.get(\"distribution\", {})\n",
    "        },\n",
    "        \"interpretation\": \"Text mining reveals prevalent issues and disclosure quality across industry\"\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6. ETHICS AGENT TOOLS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T05:49:50.602037Z",
     "iopub.status.busy": "2025-12-01T05:49:50.601686Z",
     "iopub.status.idle": "2025-12-01T05:49:50.619462Z",
     "shell.execute_reply": "2025-12-01T05:49:50.618587Z",
     "shell.execute_reply.started": "2025-12-01T05:49:50.602004Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def detect_regional_bias() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Analyzes potential geographic bias in scoring.\n",
    "    \"\"\"\n",
    "    df = GLOBAL_STATE.get(\"ktc_scoring\")\n",
    "    if df is None or \"Region\" not in df.columns:\n",
    "        return json_safe({\"status\": \"error\", \"message\": \"Regional data not available\"})\n",
    "\n",
    "    regions = df[\"Region\"].unique()\n",
    "    regional_scores: Dict[str, List[float]] = {}\n",
    "\n",
    "    for region in regions:\n",
    "        region_df = df[df[\"Region\"] == region]\n",
    "        if \"Total_Benchmark\" in region_df.columns:\n",
    "            regional_scores[region] = region_df[\"Total_Benchmark\"].tolist()\n",
    "\n",
    "    if len(regional_scores) >= 2:\n",
    "        all_scores = [score for scores in regional_scores.values() for score in scores]\n",
    "        within_region_var = float(np.mean([np.var(scores) for scores in regional_scores.values() if len(scores) > 1]))\n",
    "        total_var = float(np.var(all_scores))\n",
    "        variance_ratio = float(within_region_var / total_var if total_var > 0 else 0)\n",
    "    else:\n",
    "        variance_ratio = 0.0\n",
    "\n",
    "    regional_avg = {region: float(round(np.mean(scores), 2)) for region, scores in regional_scores.items()}\n",
    "\n",
    "    max_region = max(regional_avg.items(), key=lambda x: x[1])\n",
    "    min_region = min(regional_avg.items(), key=lambda x: x[1])\n",
    "    gap = float(max_region[1] - min_region[1])\n",
    "\n",
    "    bias_assessment = {\n",
    "        \"bias_detected\": bool(gap > 15),\n",
    "        \"max_region\": max_region,\n",
    "        \"min_region\": min_region,\n",
    "        \"gap\": float(round(gap, 2)),\n",
    "        \"variance_ratio\": float(round(variance_ratio, 3))\n",
    "    }\n",
    "\n",
    "    return json_safe({\n",
    "        \"status\": \"success\",\n",
    "        \"regional_averages\": regional_avg,\n",
    "        \"bias_assessment\": bias_assessment,\n",
    "        \"recommendation\": (\n",
    "            \"Significant regional gaps may indicate need for context-adjusted benchmarking\"\n",
    "            if gap > 15 else\n",
    "            \"Regional differences within acceptable range\"\n",
    "        )\n",
    "    })\n",
    "\n",
    "\n",
    "def check_data_completeness() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Evaluates data quality and completeness.\n",
    "    \"\"\"\n",
    "    df = GLOBAL_STATE.get(\"ktc_scoring\")\n",
    "    if df is None:\n",
    "        return json_safe({\"status\": \"error\", \"message\": \"Data not loaded\"})\n",
    "\n",
    "    score_cols = GLOBAL_STATE.get(\"score_columns\", [])\n",
    "\n",
    "    completeness_report = {}\n",
    "    for col in score_cols:\n",
    "        if col in df.columns:\n",
    "            total = len(df)\n",
    "            missing = df[col].isna().sum()\n",
    "            completeness = ((total - missing) / total * 100) if total > 0 else 0.0\n",
    "\n",
    "            completeness_report[col] = {\n",
    "                \"missing_count\": int(missing),\n",
    "                \"completeness_pct\": float(round(completeness, 1))\n",
    "            }\n",
    "\n",
    "    avg_completeness = float(np.mean([v[\"completeness_pct\"] for v in completeness_report.values()]))\n",
    "\n",
    "    return json_safe({\n",
    "        \"status\": \"success\",\n",
    "        \"completeness_by_theme\": completeness_report,\n",
    "        \"overall_completeness\": float(round(avg_completeness, 1)),\n",
    "        \"quality_flag\": \"high\" if avg_completeness > 90 else \"medium\" if avg_completeness > 75 else \"low\"\n",
    "    })\n",
    "\n",
    "\n",
    "def ethical_ai_checklist() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Evaluates ethical compliance of the AI system itself.\n",
    "    \"\"\"\n",
    "    checklist = {\n",
    "        \"data_transparency\": {\n",
    "            \"passed\": bool(GLOBAL_STATE.get(\"initialized\", False)),\n",
    "            \"note\": \"Data source and loading process documented\"\n",
    "        },\n",
    "        \"bias_detection\": {\n",
    "            \"passed\": bool(\"correlation_matrix\" in GLOBAL_STATE),\n",
    "            \"note\": \"Inter-theme correlations computed\"\n",
    "        },\n",
    "        \"model_interpretability\": {\n",
    "            \"passed\": bool(\"model_coefficients\" in GLOBAL_STATE),\n",
    "            \"note\": \"Linear regression ensures interpretable predictions\"\n",
    "        },\n",
    "        \"data_completeness\": {\n",
    "            \"passed\": True,\n",
    "            \"note\": \"Missing data patterns analyzed\"\n",
    "        },\n",
    "        \"reproducibility\": {\n",
    "            \"passed\": True,\n",
    "            \"note\": \"Random seeds fixed, process documented\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    passed_count = sum(1 for item in checklist.values() if item[\"passed\"])\n",
    "    total_count = len(checklist)\n",
    "\n",
    "    return json_safe({\n",
    "        \"status\": \"success\",\n",
    "        \"checklist\": checklist,\n",
    "        \"ethical_score\": f\"{passed_count}/{total_count}\",\n",
    "        \"recommendation\": (\n",
    "            \"AI system meets basic ethical standards\"\n",
    "            if passed_count == total_count else\n",
    "            \"Some ethical concerns require attention\"\n",
    "        )\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **7. SYNTHESIS AGENT TOOLS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T05:49:54.274625Z",
     "iopub.status.busy": "2025-12-01T05:49:54.274317Z",
     "iopub.status.idle": "2025-12-01T05:49:54.288125Z",
     "shell.execute_reply": "2025-12-01T05:49:54.287281Z",
     "shell.execute_reply.started": "2025-12-01T05:49:54.274606Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_executive_summary(language: str = \"en\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Generates comprehensive summary for stakeholders (English only).\n",
    "    \"\"\"\n",
    "    findings: Dict[str, Any] = {}\n",
    "\n",
    "    if GLOBAL_STATE.get(\"ktc_scoring\") is not None:\n",
    "        df = GLOBAL_STATE[\"ktc_scoring\"]\n",
    "        findings[\"data\"] = {\n",
    "            \"n_companies\": int(len(df)),\n",
    "            \"avg_score\": float(round(df[\"Total_Benchmark\"].mean(), 1))\n",
    "            if \"Total_Benchmark\" in df.columns else None,\n",
    "            \"top_performer\": (\n",
    "                df.nlargest(1, \"Total_Benchmark\")[\"Company_Name\"].iloc[0]\n",
    "                if \"Total_Benchmark\" in df.columns and \"Company_Name\" in df.columns else None\n",
    "            )\n",
    "        }\n",
    "\n",
    "    if \"correlation_matrix\" in GLOBAL_STATE:\n",
    "        findings[\"analysis\"] = {\n",
    "            \"correlations_computed\": True,\n",
    "            \"clustering_performed\": \"Cluster\" in GLOBAL_STATE.get(\"ktc_scoring\", pd.DataFrame()).columns\n",
    "        }\n",
    "\n",
    "    if \"model_coefficients\" in GLOBAL_STATE:\n",
    "        coeffs = GLOBAL_STATE[\"model_coefficients\"]\n",
    "        top_factor = max(coeffs.items(), key=lambda x: abs(x[1]))\n",
    "        findings[\"prediction\"] = {\n",
    "            \"model_trained\": True,\n",
    "            \"top_driver\": top_factor[0],\n",
    "            \"coefficient\": float(round(top_factor[1], 2))\n",
    "        }\n",
    "\n",
    "    findings[\"ethics\"] = {\n",
    "        \"bias_checked\": True,\n",
    "        \"data_quality_assessed\": True\n",
    "    }\n",
    "\n",
    "    summary = f\"\"\"\n",
    "    ðŸ“Š Multi-Agent AI Analysis - Executive Summary\n",
    "\n",
    "    ðŸ“ˆ Data: {findings.get('data', {}).get('n_companies', 'N/A')} companies assessed\n",
    "    ðŸ† Average Overall Score: {findings.get('data', {}).get('avg_score', 'N/A')}/100\n",
    "    â­ Top Performer: {findings.get('data', {}).get('top_performer', 'N/A')}\n",
    "\n",
    "    ðŸ” Analysis: Inter-theme correlations computed\n",
    "    ðŸ“Š Clustering: Companies grouped by performance patterns\n",
    "\n",
    "    ðŸŽ¯ Prediction: Regression model trained\n",
    "    ðŸ’¡ Key Driver: {findings.get('prediction', {}).get('top_driver', 'N/A')}\n",
    "\n",
    "    âœ… Ethics: Regional bias and data quality assessed\n",
    "    \"\"\"\n",
    "\n",
    "    return json_safe({\n",
    "        \"status\": \"success\",\n",
    "        \"summary\": summary.strip(),\n",
    "        \"findings\": findings,\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    })\n",
    "\n",
    "\n",
    "def create_visualization_data() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Prepares data for heatmaps and visualizations.\n",
    "    \"\"\"\n",
    "    if not PLOTTING_AVAILABLE:\n",
    "        return json_safe({\n",
    "            \"status\": \"limited\",\n",
    "            \"message\": \"Matplotlib not available - returning data only\",\n",
    "            \"data_for_external_plotting\": \"correlation_matrix available in GLOBAL_STATE\"\n",
    "        })\n",
    "\n",
    "    viz_data: Dict[str, Any] = {}\n",
    "\n",
    "    if \"correlation_matrix\" in GLOBAL_STATE:\n",
    "        viz_data[\"correlation_matrix\"] = GLOBAL_STATE[\"correlation_matrix\"]\n",
    "\n",
    "    df = GLOBAL_STATE.get(\"ktc_scoring\")\n",
    "    if df is not None and \"Region\" in df.columns and \"Total_Benchmark\" in df.columns:\n",
    "        regional_data = {\n",
    "            group: float(mean)\n",
    "            for group, mean in df.groupby(\"Region\")[\"Total_Benchmark\"].mean().items()\n",
    "        }\n",
    "        viz_data[\"regional_scores\"] = regional_data\n",
    "\n",
    "    if df is not None and \"Cluster\" in df.columns:\n",
    "        cluster_data = df.groupby(\"Cluster\").mean(numeric_only=True).to_dict(orient=\"index\")\n",
    "        viz_data[\"cluster_profiles\"] = {\n",
    "            k: {col: float(val) for col, val in v.items()}\n",
    "            for k, v in cluster_data.items()\n",
    "        }\n",
    "\n",
    "    return json_safe({\n",
    "        \"status\": \"success\",\n",
    "        \"visualization_data\": viz_data,\n",
    "        \"note\": \"Use seaborn.heatmap() for correlation matrix, matplotlib.bar() for regional comparison\"\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **8. AGENT SETUP WITH GOOGLE ADK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T05:49:54.290041Z",
     "iopub.status.busy": "2025-12-01T05:49:54.289711Z",
     "iopub.status.idle": "2025-12-01T05:49:54.321285Z",
     "shell.execute_reply": "2025-12-01T05:49:54.320346Z",
     "shell.execute_reply.started": "2025-12-01T05:49:54.290019Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– Initializing AI Agents...\n",
      "âœ… 7 agents initialized successfully\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ¤– Initializing AI Agents...\")\n",
    "\n",
    "tools_data_agent = [\n",
    "    FunctionTool(prepare_ktc_data),\n",
    "    FunctionTool(get_company_profile),\n",
    "    FunctionTool(get_leaderboard),\n",
    "    FunctionTool(load_detailed_research),\n",
    "    FunctionTool(load_non_scored_research),\n",
    "    FunctionTool(filter_companies_by_region),\n",
    "    FunctionTool(clean_scores_and_top_companies)\n",
    "]\n",
    "\n",
    "tools_analysis_agent = [\n",
    "    FunctionTool(compute_theme_correlations),\n",
    "    FunctionTool(perform_clustering),\n",
    "    FunctionTool(analyze_indicator_variance),\n",
    "    FunctionTool(regional_comparison),\n",
    "    FunctionTool(total_benchmark_stats),\n",
    "    FunctionTool(marketcap_totalbenchmark_correlation),\n",
    "    FunctionTool(custom_clustering),\n",
    "    FunctionTool(median_theme_scores_by_region)\n",
    "]\n",
    "\n",
    "tools_research_agent = [\n",
    "    FunctionTool(search_external_validation),\n",
    "    FunctionTool(cross_reference_allegations)\n",
    "]\n",
    "\n",
    "tools_prediction_agent = [\n",
    "    FunctionTool(train_regression_model),\n",
    "    FunctionTool(forecast_scenario),\n",
    "    FunctionTool(predict_score_improvements),\n",
    "    FunctionTool(project_industry_average),\n",
    "    FunctionTool(predict_company_rank_change),\n",
    "    FunctionTool(regional_theme_improvement)\n",
    "]\n",
    "\n",
    "tools_text_mining_agent = [\n",
    "    FunctionTool(extract_text_sections),\n",
    "    FunctionTool(perform_sentiment_analysis),\n",
    "    FunctionTool(keyword_frequency_analysis),\n",
    "    FunctionTool(text_mining_summary)\n",
    "]\n",
    "\n",
    "tools_ethics_agent = [\n",
    "    FunctionTool(detect_regional_bias),\n",
    "    FunctionTool(check_data_completeness),\n",
    "    FunctionTool(ethical_ai_checklist)\n",
    "]\n",
    "\n",
    "tools_synthesis_agent = [\n",
    "    FunctionTool(generate_executive_summary),\n",
    "    FunctionTool(create_visualization_data)\n",
    "]\n",
    "\n",
    "agent_data = LlmAgent(\n",
    "    name=\"DataAgent\",\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    tools=tools_data_agent,\n",
    "    description=\"Data extraction and preprocessing specialist\",\n",
    "    instruction=\"\"\"You are the Data Agent. Your role:\n",
    "    1. Load and clean KnowTheChain ICT benchmark data\n",
    "    2. Filter and aggregate companies by region, theme, or score\n",
    "    3. Provide company profiles, rankings, and cleaned tables\n",
    "    4. Handle data quality issues\n",
    "    Always call prepare_ktc_data() first if data not loaded.\"\"\"\n",
    ")\n",
    "\n",
    "agent_analysis = LlmAgent(\n",
    "    name=\"AnalysisAgent\",\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    tools=tools_analysis_agent,\n",
    "    description=\"Statistical analysis and pattern detection expert\",\n",
    "    instruction=\"\"\"You are the Analysis Agent. Your role:\n",
    "    1. Compute descriptive statistics (mean, std, median) and correlations\n",
    "    2. Perform clustering (k-means) to group companies\n",
    "    3. Compare regions and identify high-variance indicators\n",
    "    4. Use the dedicated tools when a query mentions 'average', 'std',\n",
    "       'correlation', 'cluster', or 'median'.\"\"\"\n",
    ")\n",
    "\n",
    "agent_research = LlmAgent(\n",
    "    name=\"ResearchAgent\",\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    tools=tools_research_agent,\n",
    "    description=\"External validation and cross-referencing specialist\",\n",
    "    instruction=\"\"\"You are the Research Agent. Your role:\n",
    "    1. Validate findings against external sources (ILO, BHRRC)\n",
    "    2. Cross-reference companies with known allegations\n",
    "    3. Provide contextual information about global forced labour risks.\"\"\"\n",
    ")\n",
    "\n",
    "agent_prediction = LlmAgent(\n",
    "    name=\"PredictionAgent\",\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    tools=tools_prediction_agent,\n",
    "    description=\"Forecasting and scenario modeling expert\",\n",
    "    instruction=\"\"\"You are the Prediction Agent. Your role:\n",
    "    1. Train regression models to identify key drivers\n",
    "    2. Run 'what-if' scenarios (e.g. Remedy +10 points)\n",
    "    3. Project future averages and hypothetical rank changes\n",
    "    Always call train_regression_model() before scenario tools if model is not ready.\"\"\"\n",
    ")\n",
    "\n",
    "agent_text_mining = LlmAgent(\n",
    "    name=\"TextMiningAgent\",\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    tools=tools_text_mining_agent,\n",
    "    description=\"Natural language processing and text analysis specialist\",\n",
    "    instruction=\"\"\"You are the Text Mining Agent. Your role:\n",
    "    1. Analyze Sheet 3 (Non-scored research) for key phrases and sentiment\n",
    "    2. Use keyword_frequency_analysis, perform_sentiment_analysis, and\n",
    "       extract_text_sections for queries mentioning 'Non-Scored Research',\n",
    "       'UK MSA', 'forced labour', 'opportunities for improvement', etc.\"\"\"\n",
    ")\n",
    "\n",
    "agent_ethics = LlmAgent(\n",
    "    name=\"EthicsAgent\",\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    tools=tools_ethics_agent,\n",
    "    description=\"Bias detection and ethical compliance auditor\",\n",
    "    instruction=\"\"\"You are the Ethics Agent. Your role:\n",
    "    1. Detect regional or other biases in data/analysis\n",
    "    2. Assess data completeness and quality\n",
    "    3. Evaluate ethical compliance of AI system itself\n",
    "    Think in terms of principal-agent problems and amplification of bias.\"\"\"\n",
    ")\n",
    "\n",
    "agent_synthesis = LlmAgent(\n",
    "    name=\"SynthesisAgent\",\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    tools=tools_synthesis_agent,\n",
    "    description=\"Report generation and synthesis specialist\",\n",
    "    instruction=\"\"\"You are the Synthesis Agent. Your role:\n",
    "    1. Compile insights from all other agents\n",
    "    2. Generate executive summaries in English\n",
    "    3. Prepare visualization data and academic-style reports.\"\"\"\n",
    ")\n",
    "\n",
    "AGENT_REGISTRY = {\n",
    "    \"data\": agent_data,\n",
    "    \"analysis\": agent_analysis,\n",
    "    \"research\": agent_research,\n",
    "    \"prediction\": agent_prediction,\n",
    "    \"text_mining\": agent_text_mining,\n",
    "    \"ethics\": agent_ethics,\n",
    "    \"synthesis\": agent_synthesis\n",
    "}\n",
    "\n",
    "print(f\"âœ… {len(AGENT_REGISTRY)} agents initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **9. ORCHESTRATION LAYER (Tools, Router, and Memory)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T06:01:54.979022Z",
     "iopub.status.busy": "2025-12-01T06:01:54.978594Z",
     "iopub.status.idle": "2025-12-01T06:01:54.988626Z",
     "shell.execute_reply": "2025-12-01T06:01:54.987464Z",
     "shell.execute_reply.started": "2025-12-01T06:01:54.978994Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "MEMORY_STATE: Dict[str, Any] = {\n",
    "    \"history\": [],          \n",
    "    \"preferences\": {\n",
    "        \"language\": \"en\"\n",
    "    }\n",
    "}\n",
    "\n",
    "def get_recent_history(n: int = 3) -> str:\n",
    "    \"\"\"\n",
    "    Returns a plaintext summary of the last n QA pairs from MEMORY_STATE.\n",
    "    Used to provide lightweight conversational context to agents.\n",
    "    \"\"\"\n",
    "    history = MEMORY_STATE.get(\"history\", [])\n",
    "    if not history:\n",
    "        return \"\"\n",
    "\n",
    "    recent = history[-n:]\n",
    "    parts = []\n",
    "    for item in recent:\n",
    "        q = item.get(\"question\", \"\")\n",
    "        a = item.get(\"final_synthesis\", \"\")\n",
    "        parts.append(f\"Q: {q}\\nA: {a}\")\n",
    "    return \"\\n\\n\".join(parts)\n",
    "\n",
    "\n",
    "def update_memory(question: str,\n",
    "                  final_synthesis: str,\n",
    "                  agents_used: List[str],\n",
    "                  language: str) -> None:\n",
    "    \"\"\"\n",
    "    Appends the current interaction to MEMORY_STATE and updates preferences.\n",
    "    \"\"\"\n",
    "    entry = {\n",
    "        \"question\": question,\n",
    "        \"final_synthesis\": final_synthesis,\n",
    "        \"agents_used\": agents_used,\n",
    "        \"language\": language,\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "    MEMORY_STATE.setdefault(\"history\", []).append(entry)\n",
    "    MEMORY_STATE.setdefault(\"preferences\", {})[\"language\"] = language\n",
    "\n",
    "\n",
    "def get_memory(n: int = 5) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Utility: returns the last n memory entries (for debugging / inspection).\n",
    "    \"\"\"\n",
    "    history = MEMORY_STATE.get(\"history\", [])\n",
    "    return history[-n:]\n",
    "\n",
    "\n",
    "def clear_memory() -> None:\n",
    "    \"\"\"\n",
    "    Utility: clears all stored conversational memory.\n",
    "    \"\"\"\n",
    "    MEMORY_STATE[\"history\"] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T05:51:40.041650Z",
     "iopub.status.busy": "2025-12-01T05:51:40.041325Z",
     "iopub.status.idle": "2025-12-01T05:51:40.076061Z",
     "shell.execute_reply": "2025-12-01T05:51:40.075022Z",
     "shell.execute_reply.started": "2025-12-01T05:51:40.041630Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:asyncio:Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x79f5188f0ad0>\n",
      "ERROR:asyncio:Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x79f5188ecd90>\n",
      "ERROR:asyncio:Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x79f518725790>\n"
     ]
    }
   ],
   "source": [
    "async def run_single_agent(agent, prompt, app_name):\n",
    "    runner = InMemoryRunner(agent=agent, app_name=app_name)\n",
    "\n",
    "    if hasattr(runner, \"session_service\"):\n",
    "        try:\n",
    "            await runner.session_service.create_session(\n",
    "                session_id=\"sess1\",\n",
    "                user_id=\"user1\",\n",
    "                app_name=app_name\n",
    "            )\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    txt = \"\"\n",
    "    async for e in runner.run_async(\n",
    "        user_id=\"user1\",\n",
    "        session_id=\"sess1\",\n",
    "        new_message=genai_types.Content(role=\"user\", parts=[genai_types.Part(text=prompt)])\n",
    "    ):\n",
    "        if e.is_final_response() and e.content:\n",
    "            txt += e.content.parts[0].text\n",
    "    return txt\n",
    "\n",
    "\n",
    "def route_query(question: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Intelligent router - determines which agents to invoke.\n",
    "\n",
    "    Key ideas:\n",
    "    - Scores each agent by how many of its keywords are present.\n",
    "    - Handles special \"full report\" / \"summary\" intents.\n",
    "    - Supports explicit agent mentions (e.g. \"use data agent\").\n",
    "    - Ensures DataAgent runs at least once if data is not initialized.\n",
    "    - Only adds SynthesisAgent when the user explicitly asks for a summary/report\n",
    "      OR when many specialized agents are involved.\n",
    "    \"\"\"\n",
    "    q_lower = question.lower()\n",
    "    agents_to_run: List[str] = []\n",
    "\n",
    "    # 1) Explicit agent mentions (highest priority)\n",
    "    explicit_map = {\n",
    "        \"data\": [\"data agent\"],\n",
    "        \"analysis\": [\"analysis agent\"],\n",
    "        \"research\": [\"research agent\"],\n",
    "        \"prediction\": [\"prediction agent\"],\n",
    "        \"text_mining\": [\"text mining agent\", \"text-mining agent\", \"nlp agent\"],\n",
    "        \"ethics\": [\"ethics agent\"],\n",
    "        \"synthesis\": [\"synthesis agent\"]\n",
    "    }\n",
    "    for agent_name, triggers in explicit_map.items():\n",
    "        if any(trigger in q_lower for trigger in triggers):\n",
    "            agents_to_run.append(agent_name)\n",
    "\n",
    "    # 2) Sheet / structure based routing (next priority)\n",
    "    if \"non-scored research\" in q_lower or \"non scored research\" in q_lower:\n",
    "        agents_to_run.extend([\"data\", \"text_mining\"])\n",
    "    if \"detailed scoring\" in q_lower or \"detailed scoring &\" in q_lower:\n",
    "        agents_to_run.extend([\"data\", \"analysis\", \"text_mining\"])\n",
    "    if \"scoring sheet\" in q_lower or \"sheet 1\" in q_lower:\n",
    "        agents_to_run.append(\"data\")\n",
    "\n",
    "    # 3) Special high-level intents\n",
    "    if \"full benchmark report\" in q_lower or \"comprehensive report\" in q_lower:\n",
    "        agents_to_run.extend([\"data\", \"analysis\", \"prediction\", \"text_mining\", \"ethics\", \"synthesis\"])\n",
    "    explicit_summary_intent = any(\n",
    "        kw in q_lower\n",
    "        for kw in [\"summary\", \"executive summary\", \"overview\", \"final report\"]\n",
    "    )\n",
    "\n",
    "    # 4) Keyword-based scoring\n",
    "    routing_signals: Dict[str, Dict[str, List[str]]] = {\n",
    "        \"data\": {\n",
    "            \"core\": [\n",
    "                \"data\", \"dataset\", \"company\", \"companies\", \"profile\", \"profiles\",\n",
    "                \"list\", \"table\", \"rank\", \"ranking\", \"leaderboard\", \"market cap\",\n",
    "                \"region\", \"country\", \"sheet\", \"scoring\", \"non-scored\", \"benchmark\"\n",
    "            ]\n",
    "        },\n",
    "        \"analysis\": {\n",
    "            \"core\": [\n",
    "                \"analysis\", \"correlation\", \"correlate\", \"cluster\", \"clustering\",\n",
    "                \"compare\", \"comparison\", \"relationship\", \"variance\", \"difference\",\n",
    "                \"distribution\", \"average\", \"mean\", \"std\", \"standard deviation\",\n",
    "                \"median\", \"pattern\", \"trend\", \"calculate\", \"compute\"\n",
    "            ]\n",
    "        },\n",
    "        \"research\": {\n",
    "            \"core\": [\n",
    "                \"research\", \"paper\", \"study\", \"literature\", \"ilo\",\n",
    "                \"allegation\", \"allegations\", \"external source\", \"external data\",\n",
    "                \"news\", \"report\", \"resource centre\", \"bhrrc\"\n",
    "            ]\n",
    "        },\n",
    "        \"prediction\": {\n",
    "            \"core\": [\n",
    "                \"predict\", \"prediction\", \"forecast\", \"scenario\",\n",
    "                \"what if\", \"what-if\", \"model\", \"impact\", \"project\",\n",
    "                \"projection\", \"future\", \"simulate\", \"simulation\",\n",
    "                \"improvement\", \"improvements\"\n",
    "            ]\n",
    "        },\n",
    "        \"text_mining\": {\n",
    "            \"core\": [\n",
    "                \"text\", \"sentiment\", \"keyword\", \"keywords\", \"phrase\", \"phrases\",\n",
    "                \"section\", \"sections\", \"msa\", \"uk msa\", \"forced labour\",\n",
    "                \"pdf\", \"non-scored\", \"opportunit\"\n",
    "            ]\n",
    "        },\n",
    "        \"ethics\": {\n",
    "            \"core\": [\n",
    "                \"ethics\", \"ethical\", \"bias\", \"biased\", \"fairness\",\n",
    "                \"quality\", \"data quality\", \"completeness\",\n",
    "                \"principal-agent\", \"responsible ai\", \"responsible a.i.\"\n",
    "            ]\n",
    "        },\n",
    "        \"synthesis\": {\n",
    "            \"core\": [\n",
    "                \"summary\", \"final report\", \"overview\", \"synthesise\",\n",
    "                \"synthesize\", \"high-level\", \"executive summary\", \"conclusion\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    scores: Dict[str, int] = {name: 0 for name in routing_signals.keys()}\n",
    "\n",
    "    for agent_name, cfg in routing_signals.items():\n",
    "        for kw in cfg[\"core\"]:\n",
    "            if kw in q_lower:\n",
    "                scores[agent_name] += 1\n",
    "\n",
    "    # 5) Add agents with positive scores, ordered by score (highest first)\n",
    "    scored_agents_sorted = sorted(\n",
    "        [a for a, s in scores.items() if s > 0],\n",
    "        key=lambda a: scores[a],\n",
    "        reverse=True\n",
    "    )\n",
    "    agents_to_run.extend(scored_agents_sorted)\n",
    "\n",
    "    # 6) Ensure DataAgent runs at least once if data not initialized\n",
    "    if not GLOBAL_STATE.get(\"initialized\"):\n",
    "        agents_to_run.append(\"data\")\n",
    "\n",
    "    # 7) Fallback: if still nothing, use data + analysis as safe default\n",
    "    if not agents_to_run:\n",
    "        agents_to_run = [\"data\", \"analysis\"]\n",
    "\n",
    "    # 8) Deduplicate while preserving order\n",
    "    agents_to_run = list(dict.fromkeys(agents_to_run))\n",
    "\n",
    "    # 9) Decide whether we REALLY need SynthesisAgent\n",
    "    non_utility_agents = [a for a in agents_to_run if a not in (\"data\",)]\n",
    "    many_specialists = len(non_utility_agents) >= 3\n",
    "    needs_synthesis = explicit_summary_intent or many_specialists\n",
    "\n",
    "    if needs_synthesis and \"synthesis\" not in agents_to_run:\n",
    "        agents_to_run.append(\"synthesis\")\n",
    "    else:\n",
    "        if not needs_synthesis and \"synthesis\" in agents_to_run:\n",
    "            agents_to_run = [a for a in agents_to_run if a != \"synthesis\"]\n",
    "\n",
    "    return agents_to_run\n",
    "\n",
    "\n",
    "async def execute_multi_agent_query(question: str,\n",
    "                                    language: str = \"en\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Main orchestration function - coordinates all agents.\n",
    "    Uses MEMORY_STATE to:\n",
    "    - Inject recent conversation history into the context.\n",
    "    - Persist the current interaction for future queries.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"ðŸš€ Starting Multi-Agent Analysis\")\n",
    "    print(f\"ðŸ“ Query: {question}\")\n",
    "    print(f\"{'=' * 60}\\n\")\n",
    "\n",
    "    # Respect stored language preference if language is None\n",
    "    if language is None:\n",
    "        language = MEMORY_STATE.get(\"preferences\", {}).get(\"language\", \"en\")\n",
    "\n",
    "    agents_needed = route_query(question)\n",
    "    print(f\"ðŸŽ¯ Agents selected: {', '.join(agents_needed)}\\n\")\n",
    "\n",
    "    agent_outputs: Dict[str, Any] = {}\n",
    "\n",
    "    # Build initial context from recent memory + current question\n",
    "    recent_context = get_recent_history(n=3)\n",
    "    if recent_context:\n",
    "        context_accumulated = f\"Recent conversation context:\\n{recent_context}\\n\\n\"\n",
    "    else:\n",
    "        context_accumulated = \"\"\n",
    "    context_accumulated += f\"User Question: {question}\\n\\n\"\n",
    "\n",
    "    for agent_name in agents_needed:\n",
    "        print(f\"â–¶ï¸ Running {agent_name.upper()} agent...\")\n",
    "\n",
    "        agent = AGENT_REGISTRY[agent_name]\n",
    "\n",
    "        if agent_name == \"synthesis\":\n",
    "            prompt = f\"\"\"Synthesize findings from other agents:\n",
    "\n",
    "{context_accumulated}\n",
    "Please provide a concise answer to the user's question in **{language}**.\n",
    "Focus on directly answering the question first, then (briefly) highlight key insights.\"\"\"\n",
    "        else:\n",
    "            prompt = f\"{question}\\n\\nContext from previous agents:\\n{context_accumulated}\"\n",
    "\n",
    "        try:\n",
    "            response = await run_single_agent(\n",
    "                agent=agent,\n",
    "                prompt=prompt,\n",
    "                app_name=f\"ict_benchmark_{agent_name}\"\n",
    "            )\n",
    "\n",
    "            agent_outputs[agent_name] = {\n",
    "                \"status\": \"success\",\n",
    "                \"response\": response\n",
    "            }\n",
    "            context_accumulated += f\"\\n--- {agent_name.upper()} FINDINGS ---\\n{response}\\n\"\n",
    "            print(f\" âœ… {agent_name} completed\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" âŒ {agent_name} failed: {str(e)}\\n\")\n",
    "            agent_outputs[agent_name] = {\n",
    "                \"status\": \"error\",\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "\n",
    "    print(f\"{'=' * 60}\")\n",
    "    print(f\"âœ… Multi-Agent Analysis Complete\")\n",
    "    print(f\"{'=' * 60}\\n\")\n",
    "\n",
    "    if \"synthesis\" in agent_outputs and agent_outputs[\"synthesis\"].get(\"status\") == \"success\":\n",
    "        final_text = agent_outputs[\"synthesis\"].get(\"response\", \"\")\n",
    "    else:\n",
    "        primary_text = None\n",
    "        for name in agents_needed:\n",
    "            if name in (\"data\", \"synthesis\"):\n",
    "                continue\n",
    "            out = agent_outputs.get(name)\n",
    "            if out and out.get(\"status\") == \"success\" and out.get(\"response\"):\n",
    "                primary_text = out[\"response\"]\n",
    "                break\n",
    "\n",
    "        if primary_text is not None:\n",
    "            final_text = primary_text\n",
    "        else:\n",
    "            fallback_text = \"\"\n",
    "            for name, out in agent_outputs.items():\n",
    "                if out.get(\"status\") == \"success\" and out.get(\"response\"):\n",
    "                    fallback_text = out[\"response\"]\n",
    "                    break\n",
    "            final_text = fallback_text or \"No synthesis available\"\n",
    "\n",
    "    update_memory(\n",
    "        question=question,\n",
    "        final_synthesis=final_text,\n",
    "        agents_used=agents_needed,\n",
    "        language=language\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"language\": language,\n",
    "        \"agents_used\": agents_needed,\n",
    "        \"agent_outputs\": agent_outputs,\n",
    "        \"final_synthesis\": final_text,\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "\n",
    "async def smart_answer(question: str, language: str = \"en\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Convenience wrapper for notebooks: routes the query through the full\n",
    "    multi-agent pipeline and returns the final synthesis plus per-agent outputs.\n",
    "    If language is None, uses the last stored preference in MEMORY_STATE.\n",
    "    \"\"\"\n",
    "    print(f\"\\nðŸ¤– Query: {question}\")\n",
    "    result = await execute_multi_agent_query(question, language=language)\n",
    "    return {\n",
    "        \"final_report\": result.get(\"final_synthesis\"),\n",
    "        \"agent_outputs\": result.get(\"agent_outputs\"),\n",
    "        \"agents_used\": result.get(\"agents_used\"),\n",
    "        \"timestamp\": result.get(\"timestamp\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T05:42:47.993863Z",
     "iopub.status.busy": "2025-12-01T05:42:47.993519Z",
     "iopub.status.idle": "2025-12-01T05:42:55.499415Z",
     "shell.execute_reply": "2025-12-01T05:42:55.498492Z",
     "shell.execute_reply.started": "2025-12-01T05:42:47.993778Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ¤– Query: Filter the companies in the Scoring sheet by Region = Asia, and list their Company Names and Total Benchmark Scores.\n",
      "\n",
      "============================================================\n",
      "ðŸš€ Starting Multi-Agent Analysis\n",
      "ðŸ“ Query: Filter the companies in the Scoring sheet by Region = Asia, and list their Company Names and Total Benchmark Scores.\n",
      "============================================================\n",
      "\n",
      "ðŸŽ¯ Agents selected: data\n",
      "\n",
      "â–¶ï¸ Running DATA agent...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call', 'thought_signature'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š DATA AGENT: Loading KnowTheChain 2025 ICT Benchmark...\n",
      " âœ“ Loaded 47 rows from Sheet 1\n",
      " âœ… data completed\n",
      "\n",
      "============================================================\n",
      "âœ… Multi-Agent Analysis Complete\n",
      "============================================================\n",
      "\n",
      "Here are the companies in Asia and their Total Benchmark Scores:\n",
      "\n",
      "*   **Samsung Electronics Co. Ltd.**: 61\n",
      "*   **LG Electronics Inc.**: 24\n",
      "*   **SK Hynix Inc.** : 20\n",
      "*   **Sony Corp.**: 20\n",
      "*   **Hon Hai Precision Industry Co. Ltd. (Foxconn)**: 19\n",
      "*   **Taiwan Semiconductor Manufacturing Co. Ltd.**: 19\n",
      "*   **Canon Inc.**: 13\n",
      "*   **Panasonic Corp.**: 10\n",
      "*   **Murata Manufacturing Co. Ltd.**: 10\n",
      "*   **Fujifilm Holdings Corp.**: 9\n",
      "*   **Keyence Corp.**: 7\n",
      "*   **Kyocera Corp.**: 6\n",
      "*   **Xiaomi Corp.**: 5\n",
      "*   **Luxshare Precision Industry Co. Ltd.**: 4\n",
      "*   **Semiconductor Manufacturing International Corp**: 3\n",
      "*   **BOE Technology Group Co. Ltd.**: 0\n",
      "\n",
      "ðŸ¤– Query: Explain first company more\n",
      "\n",
      "============================================================\n",
      "ðŸš€ Starting Multi-Agent Analysis\n",
      "ðŸ“ Query: Explain first company more\n",
      "============================================================\n",
      "\n",
      "ðŸŽ¯ Agents selected: data\n",
      "\n",
      "â–¶ï¸ Running DATA agent...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call', 'thought_signature'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " âœ… data completed\n",
      "\n",
      "============================================================\n",
      "âœ… Multi-Agent Analysis Complete\n",
      "============================================================\n",
      "\n",
      "Samsung Electronics Co. Ltd. is based in South Korea and is a leader in the Technology Hardware Storage & Peripherals sub-industry. They ranked 1st out of 45 companies overall.\n",
      "\n",
      "Here are their scores across different categories:\n",
      "*   **Total Benchmark**: 61\n",
      "*   **Commitment & Governance**: 79.5\n",
      "*   **Enabling Workers**: 40\n",
      "*   **Monitoring**: 50\n",
      "*   **Purchasing Practices**: 34\n",
      "*   **Recruitment**: 84\n",
      "*   **Remedy**: 40\n",
      "*   **Traceability & Risk**: 69\n",
      "\n",
      "ðŸ¤– Query: Explain second company more\n",
      "\n",
      "============================================================\n",
      "ðŸš€ Starting Multi-Agent Analysis\n",
      "ðŸ“ Query: Explain second company more\n",
      "============================================================\n",
      "\n",
      "ðŸŽ¯ Agents selected: data\n",
      "\n",
      "â–¶ï¸ Running DATA agent...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call', 'thought_signature'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " âœ… data completed\n",
      "\n",
      "============================================================\n",
      "âœ… Multi-Agent Analysis Complete\n",
      "============================================================\n",
      "\n",
      "LG Electronics Inc. is based in South Korea and operates in the Consumer Electronics sub-industry. They ranked 12th out of 45 companies overall.\n",
      "\n",
      "Here are their scores across different categories:\n",
      "*   **Total Benchmark**: 24\n",
      "*   **Commitment & Governance**: 75.5\n",
      "*   **Enabling Workers**: 0\n",
      "*   **Monitoring**: 5\n",
      "*   **Purchasing Practices**: 0\n",
      "*   **Recruitment**: 15\n",
      "*   **Remedy**: 13\n",
      "*   **Traceability & Risk**: 29\n"
     ]
    }
   ],
   "source": [
    "res = await smart_answer(\"Filter the companies in the Scoring sheet by Region = Asia, and list their Company Names and Total Benchmark Scores.\")\n",
    "print(res[\"final_report\"])\n",
    "\n",
    "res = await smart_answer(\"Explain first company more\")\n",
    "print(res[\"final_report\"])\n",
    "\n",
    "res = await smart_answer(\"Explain second company more\")\n",
    "print(res[\"final_report\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Web ADK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T06:36:28.995375Z",
     "iopub.status.busy": "2025-12-01T06:36:28.995003Z",
     "iopub.status.idle": "2025-12-01T06:36:29.186144Z",
     "shell.execute_reply": "2025-12-01T06:36:29.185330Z",
     "shell.execute_reply.started": "2025-12-01T06:36:28.995350Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ADK web helper defined.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"padding: 15px; border: 2px solid #f0ad4e; border-radius: 8px; background-color: #fef9f0; margin: 20px 0;\">\n",
       "        <div style=\"font-family: sans-serif; margin-bottom: 12px; color: #333; font-size: 1.1em;\">\n",
       "            <strong>âš ï¸ IMPORTANT: Action Required</strong>\n",
       "        </div>\n",
       "        <div style=\"font-family: sans-serif; margin-bottom: 15px; color: #333; line-height: 1.5;\">\n",
       "            The ADK web UI is <strong>not running yet</strong>. You must start it in the next cell.\n",
       "            <ol style=\"margin-top: 10px; padding-left: 20px;\">\n",
       "                <li style=\"margin-bottom: 5px;\"><strong>Run the next cell</strong> (the one with <code>!adk web ...</code>) to start the ADK web UI.</li>\n",
       "                <li style=\"margin-bottom: 5px;\">Wait for that cell to stay in \"Running\" state (it will not complete).</li>\n",
       "                <li>Once it is running, <strong>come back here and click this button</strong> to open the UI.</li>\n",
       "            </ol>\n",
       "            <em style=\"font-size: 0.9em; color: #555;\">If you click the button before starting the web server, you will see an error.</em>\n",
       "        </div>\n",
       "        <a href=\"https://kkb-production.jupyter-proxy.kaggle.net/k/283029632/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2IiwidHlwIjoiSldUIn0..HkU6mYisEI0yUm6hBUCKVQ._4TYZgDeeHDermbbhyjK_uokXY5bN5iWTql9tuEIr-am6IFlGgT9AdzA2r7H_phltqfgJyl6ze8GObNhzOZg6Is4IQZnEPnQ6JcUS0tVR9VpheC2WpVCQ8f72wTRxrjBUbro4dFFxrs_CLUcjW5HXQ5JiaBfE6xOmYFdc_e40B2F9sPIkV9AocYRul4V3wcMj_JdbTdiH_NDiMoLToiEehH5wK1EL2vU5WqAVV9aRbm525NSE_FHRrSWTZPjDm_4.fj7UNy1Fpr0O0h_blJUDrQ/proxy/proxy/8000\" target=\"_blank\" style=\"\n",
       "            display: inline-block; background-color: #1a73e8; color: white; padding: 10px 20px;\n",
       "            text-decoration: none; border-radius: 25px; font-family: sans-serif; font-weight: 500;\n",
       "            box-shadow: 0 2px 5px rgba(0,0,0,0.2); transition: all 0.2s ease;\">\n",
       "            Open ADK Web UI (after running next cell) â†—\n",
       "        </a>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "from jupyter_server.serverapp import list_running_servers\n",
    "\n",
    "def get_adk_proxy_url():\n",
    "    \"\"\"\n",
    "    Build the proxied URL for the ADK web UI in a Kaggle / Jupyter environment\n",
    "    and render a button to open it.\n",
    "    \"\"\"\n",
    "    PROXY_HOST = \"https://kkb-production.jupyter-proxy.kaggle.net\"\n",
    "    ADK_PORT = \"8000\"\n",
    "\n",
    "    servers = list(list_running_servers())\n",
    "    if not servers:\n",
    "        raise RuntimeError(\"No running Jupyter servers found.\")\n",
    "\n",
    "    base_url = servers[0][\"base_url\"]\n",
    "\n",
    "    try:\n",
    "        # Expected pattern: /k/<kernel>/<token>/\n",
    "        parts = base_url.strip(\"/\").split(\"/\")\n",
    "        kernel = parts[1]\n",
    "        token = parts[2]\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Could not parse kernel/token from base URL: {base_url}\") from e\n",
    "\n",
    "    # This prefix is what ADK expects when running behind a proxy\n",
    "    url_prefix = f\"/k/{kernel}/{token}/proxy/proxy/{ADK_PORT}\"\n",
    "    full_url = f\"{PROXY_HOST}{url_prefix}\"\n",
    "\n",
    "    styled_html = f\"\"\"\n",
    "    <div style=\"padding: 15px; border: 2px solid #f0ad4e; border-radius: 8px; background-color: #fef9f0; margin: 20px 0;\">\n",
    "        <div style=\"font-family: sans-serif; margin-bottom: 12px; color: #333; font-size: 1.1em;\">\n",
    "            <strong>âš ï¸ IMPORTANT: Action Required</strong>\n",
    "        </div>\n",
    "        <div style=\"font-family: sans-serif; margin-bottom: 15px; color: #333; line-height: 1.5;\">\n",
    "            The ADK web UI is <strong>not running yet</strong>. You must start it in the next cell.\n",
    "            <ol style=\"margin-top: 10px; padding-left: 20px;\">\n",
    "                <li style=\"margin-bottom: 5px;\"><strong>Run the next cell</strong> (the one with <code>!adk web ...</code>) to start the ADK web UI.</li>\n",
    "                <li style=\"margin-bottom: 5px;\">Wait for that cell to stay in \"Running\" state (it will not complete).</li>\n",
    "                <li>Once it is running, <strong>come back here and click this button</strong> to open the UI.</li>\n",
    "            </ol>\n",
    "            <em style=\"font-size: 0.9em; color: #555;\">If you click the button before starting the web server, you will see an error.</em>\n",
    "        </div>\n",
    "        <a href=\"{full_url}\" target=\"_blank\" style=\"\n",
    "            display: inline-block; background-color: #1a73e8; color: white; padding: 10px 20px;\n",
    "            text-decoration: none; border-radius: 25px; font-family: sans-serif; font-weight: 500;\n",
    "            box-shadow: 0 2px 5px rgba(0,0,0,0.2); transition: all 0.2s ease;\">\n",
    "            Open ADK Web UI (after running next cell) â†—\n",
    "        </a>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "\n",
    "    display(HTML(styled_html))\n",
    "    return url_prefix\n",
    "\n",
    "\n",
    "print(\"âœ… ADK web helper defined.\")\n",
    "\n",
    "# Call this once to show the button and get the prefix\n",
    "url_prefix = get_adk_proxy_url()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T07:15:29.776955Z",
     "iopub.status.busy": "2025-12-01T07:15:29.776510Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/google/adk/cli/fast_api.py:130: UserWarning: [EXPERIMENTAL] InMemoryCredentialService: This feature is experimental and may change or be removed in future versions without notice. It may introduce breaking changes at any time.\n",
      "  credential_service = InMemoryCredentialService()\n",
      "/usr/local/lib/python3.11/dist-packages/google/adk/auth/credential_service/in_memory_credential_service.py:33: UserWarning: [EXPERIMENTAL] BaseCredentialService: This feature is experimental and may change or be removed in future versions without notice. It may introduce breaking changes at any time.\n",
      "  super().__init__()\n",
      "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m144\u001b[0m]\n",
      "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
      "\u001b[32m\n",
      "+-----------------------------------------------------------------------------+\n",
      "| ADK Web Server started                                                      |\n",
      "|                                                                             |\n",
      "| For local testing, access at http://127.0.0.1:8000.                         |\n",
      "+-----------------------------------------------------------------------------+\n",
      "\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
      "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://127.0.0.1:8000\u001b[0m (Press CTRL+C to quit)\n",
      "\u001b[32mINFO\u001b[0m:     35.191.50.10:0 - \"\u001b[1mGET / HTTP/1.1\u001b[0m\" \u001b[33m307 Temporary Redirect\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     35.191.50.10:0 - \"\u001b[1mGET /dev-ui/ HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     35.191.50.11:0 - \"\u001b[1mGET /dev-ui/chunk-2WH2EVR6.js HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     35.191.102.140:0 - \"\u001b[1mGET /dev-ui/polyfills-B6TNHZQ6.js HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     35.191.50.9:0 - \"\u001b[1mGET /dev-ui/main-OS2OH2S3.js HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     35.191.50.10:0 - \"\u001b[1mGET /dev-ui/styles-EVMPSV3U.css HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     35.191.50.10:0 - \"\u001b[1mGET /dev-ui/assets/config/runtime-config.json HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     35.191.50.9:0 - \"\u001b[1mGET /dev-ui/adk_favicon.svg HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     35.191.50.11:0 - \"\u001b[1mGET /list-apps?relative_path=./ HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     35.191.50.9:0 - \"\u001b[1mGET /dev-ui/assets/ADK-512-color.svg HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     35.191.50.8:0 - \"\u001b[1mPOST /builder/save?tmp=true HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO:google_adk.google.adk.cli.adk_web_server:New session created: c7b57d39-768b-44fb-9470-6f78959a70de\n",
      "\u001b[32mINFO\u001b[0m:     35.191.50.10:0 - \"\u001b[1mPOST /apps/__adk_agent_builder_assistant/users/user/sessions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     35.191.102.140:0 - \"\u001b[1mGET /dev-ui/?app=test&mode=builder HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'alias' attribute with value 'appName' was provided to the `Field()` function, which has no effect in the context it was used. 'alias' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'validation_alias' attribute with value 'appName' was provided to the `Field()` function, which has no effect in the context it was used. 'validation_alias' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'serialization_alias' attribute with value 'appName' was provided to the `Field()` function, which has no effect in the context it was used. 'serialization_alias' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'alias' attribute with value 'userId' was provided to the `Field()` function, which has no effect in the context it was used. 'alias' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'validation_alias' attribute with value 'userId' was provided to the `Field()` function, which has no effect in the context it was used. 'validation_alias' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'serialization_alias' attribute with value 'userId' was provided to the `Field()` function, which has no effect in the context it was used. 'serialization_alias' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'alias' attribute with value 'sessionId' was provided to the `Field()` function, which has no effect in the context it was used. 'alias' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'validation_alias' attribute with value 'sessionId' was provided to the `Field()` function, which has no effect in the context it was used. 'validation_alias' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'serialization_alias' attribute with value 'sessionId' was provided to the `Field()` function, which has no effect in the context it was used. 'serialization_alias' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'alias' attribute with value 'newMessage' was provided to the `Field()` function, which has no effect in the context it was used. 'alias' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'validation_alias' attribute with value 'newMessage' was provided to the `Field()` function, which has no effect in the context it was used. 'validation_alias' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'serialization_alias' attribute with value 'newMessage' was provided to the `Field()` function, which has no effect in the context it was used. 'serialization_alias' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'alias' attribute with value 'streaming' was provided to the `Field()` function, which has no effect in the context it was used. 'alias' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'validation_alias' attribute with value 'streaming' was provided to the `Field()` function, which has no effect in the context it was used. 'validation_alias' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'serialization_alias' attribute with value 'streaming' was provided to the `Field()` function, which has no effect in the context it was used. 'serialization_alias' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'alias' attribute with value 'stateDelta' was provided to the `Field()` function, which has no effect in the context it was used. 'alias' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'validation_alias' attribute with value 'stateDelta' was provided to the `Field()` function, which has no effect in the context it was used. 'validation_alias' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'serialization_alias' attribute with value 'stateDelta' was provided to the `Field()` function, which has no effect in the context it was used. 'serialization_alias' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'alias' attribute with value 'invocationId' was provided to the `Field()` function, which has no effect in the context it was used. 'alias' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'validation_alias' attribute with value 'invocationId' was provided to the `Field()` function, which has no effect in the context it was used. 'validation_alias' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'serialization_alias' attribute with value 'invocationId' was provided to the `Field()` function, which has no effect in the context it was used. 'serialization_alias' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "\u001b[32mINFO\u001b[0m:     35.191.50.11:0 - \"\u001b[1mPOST /run_sse HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO:google_adk.google.adk.cli.utils.agent_loader:Found root_agent in adk_agent_builder_assistant.agent\n",
      "INFO:google_adk.google.adk.models.google_llm:Sending out request, model: gemini-2.5-flash, backend: GoogleLLMVariant.GEMINI_API, stream: False\n",
      "\u001b[32mINFO\u001b[0m:     35.191.50.9:0 - \"\u001b[1mGET /dev-ui/assets/config/runtime-config.json HTTP/1.1\u001b[0m\" \u001b[33m304 Not Modified\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     35.191.102.140:0 - \"\u001b[1mGET /list-apps?relative_path=./ HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO:google_adk.google.adk.cli.adk_web_server:New session created: b656a1c2-4f67-4bec-9627-9992a54b3cb8\n",
      "\u001b[32mINFO\u001b[0m:     35.191.50.10:0 - \"\u001b[1mPOST /apps/test/users/user/sessions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     35.191.50.11:0 - \"\u001b[1mGET /builder/app/test?ts=1764573395150&tmp=true HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     35.191.50.8:0 - \"\u001b[1mGET /builder/app/test?ts=1764573395155 HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     35.191.50.9:0 - \"\u001b[1mGET /builder/app/test?ts=1764573395149 HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     35.191.50.10:0 - \"\u001b[1mGET /apps/test/eval_sets HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     35.191.50.9:0 - \"\u001b[1mGET /apps/test/eval_results HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     35.191.50.8:0 - \"\u001b[1mGET /apps/test/users/user/sessions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     35.191.50.11:0 - \"\u001b[1mGET /apps/test/users/user/sessions/b656a1c2-4f67-4bec-9627-9992a54b3cb8 HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     35.191.50.10:0 - \"\u001b[1mGET /apps/test/users/user/sessions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     35.191.50.9:0 - \"\u001b[1mGET /debug/trace/session/b656a1c2-4f67-4bec-9627-9992a54b3cb8 HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO:google_adk.google.adk.models.google_llm:Response received from the model.\n",
      "\u001b[32mINFO\u001b[0m:     35.191.50.106:0 - \"\u001b[1mPOST /run_sse HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "/usr/local/lib/python3.11/dist-packages/google/adk/cli/utils/agent_loader.py:225: UserWarning: [EXPERIMENTAL] _load_from_yaml_config: This feature is experimental and may change or be removed in future versions without notice. It may introduce breaking changes at any time.\n",
      "  if root_agent := self._load_from_yaml_config(actual_agent_name, agents_dir):\n",
      "/usr/local/lib/python3.11/dist-packages/google/adk/cli/utils/agent_loader.py:169: UserWarning: [EXPERIMENTAL] from_config: This feature is experimental and may change or be removed in future versions without notice. It may introduce breaking changes at any time.\n",
      "  agent = config_agent_utils.from_config(config_path)\n",
      "ERROR:google_adk.google.adk.cli.adk_web_server:Error in event_generator: No root_agent found for 'test'. Searched in 'test.agent.root_agent', 'test.root_agent' and 'test/root_agent.yaml'.\n",
      "\n",
      "Expected directory structure:\n",
      "  <agents_dir>/\n",
      "    test/\n",
      "      agent.py (with root_agent) OR\n",
      "      root_agent.yaml\n",
      "\n",
      "Then run: adk web <agents_dir>\n",
      "\n",
      "Ensure '/kaggle/working/test' is structured correctly, an .env file can be loaded if present, and a root_agent is exposed.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/google/adk/cli/adk_web_server.py\", line 1410, in event_generator\n",
      "    runner = await self.get_runner_async(req.app_name)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/google/adk/cli/adk_web_server.py\", line 472, in get_runner_async\n",
      "    agent_or_app = self.agent_loader.load_agent(app_name)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/google/adk/cli/utils/agent_loader.py\", line 306, in load_agent\n",
      "    agent_or_app = self._perform_load(agent_name)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/google/adk/cli/utils/agent_loader.py\", line 249, in _perform_load\n",
      "    raise ValueError(\n",
      "ValueError: No root_agent found for 'test'. Searched in 'test.agent.root_agent', 'test.root_agent' and 'test/root_agent.yaml'.\n",
      "\n",
      "Expected directory structure:\n",
      "  <agents_dir>/\n",
      "    test/\n",
      "      agent.py (with root_agent) OR\n",
      "      root_agent.yaml\n",
      "\n",
      "Then run: adk web <agents_dir>\n",
      "\n",
      "Ensure '/kaggle/working/test' is structured correctly, an .env file can be loaded if present, and a root_agent is exposed.\n",
      "\u001b[32mINFO\u001b[0m:     35.191.50.104:0 - \"\u001b[1mGET /apps/test/users/user/sessions/b656a1c2-4f67-4bec-9627-9992a54b3cb8 HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     35.191.50.106:0 - \"\u001b[1mGET /debug/trace/session/b656a1c2-4f67-4bec-9627-9992a54b3cb8 HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     35.191.50.105:0 - \"\u001b[1mGET /debug/trace/session/b656a1c2-4f67-4bec-9627-9992a54b3cb8 HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!adk web --url_prefix {url_prefix}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **More examples and tests**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Agent: Extracts and cleans data (e.g., filtering by region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T19:26:32.498673Z",
     "iopub.status.busy": "2025-11-30T19:26:32.498312Z",
     "iopub.status.idle": "2025-11-30T19:26:34.709384Z",
     "shell.execute_reply": "2025-11-30T19:26:34.708567Z",
     "shell.execute_reply.started": "2025-11-30T19:26:32.498643Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ¤– Query: Filter the companies in the Scoring sheet by Region = Asia, and list their Company Names and Total Benchmark Scores.\n",
      "\n",
      "============================================================\n",
      "ðŸš€ Starting Multi-Agent Analysis\n",
      "ðŸ“ Query: Filter the companies in the Scoring sheet by Region = Asia, and list their Company Names and Total Benchmark Scores.\n",
      "============================================================\n",
      "\n",
      "ðŸŽ¯ Agents selected: data\n",
      "\n",
      "â–¶ï¸ Running DATA agent...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call', 'thought_signature'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " âœ… data completed\n",
      "\n",
      "============================================================\n",
      "âœ… Multi-Agent Analysis Complete\n",
      "============================================================\n",
      "\n",
      "Here are the companies in the Asia region along with their Total Benchmark Scores:\n",
      "\n",
      "*   Samsung Electronics Co. Ltd.: 61\n",
      "*   LG Electronics Inc.: 24\n",
      "*   SK Hynix Inc.: 20\n",
      "*   Sony Corp.: 20\n",
      "*   Hon Hai Precision Industry Co. Ltd. (Foxconn): 19\n",
      "*   Taiwan Semiconductor Manufacturing Co. Ltd.: 19\n",
      "*   Canon Inc.: 13\n",
      "*   Panasonic Corp.: 10\n",
      "*   Murata Manufacturing Co. Ltd.: 10\n",
      "*   Fujifilm Holdings Corp.: 9\n",
      "*   Keyence Corp.: 7\n",
      "*   Kyocera Corp.: 6\n",
      "*   Xiaomi Corp.: 5\n",
      "*   Luxshare Precision Industry Co. Ltd.: 4\n",
      "*   Semiconductor Manufacturing International Corp: 3\n",
      "*   BOE Technology Group Co. Ltd.: 0\n"
     ]
    }
   ],
   "source": [
    "res = await smart_answer(\"Filter the companies in the Scoring sheet by Region = Asia, and list their Company Names and Total Benchmark Scores.\")\n",
    "print(res[\"final_report\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T18:10:42.369526Z",
     "iopub.status.busy": "2025-11-30T18:10:42.369171Z",
     "iopub.status.idle": "2025-11-30T18:10:44.534076Z",
     "shell.execute_reply": "2025-11-30T18:10:44.533203Z",
     "shell.execute_reply.started": "2025-11-30T18:10:42.369503Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ¤– Query: Clean the Scoring sheet by removing any rows with missing or zero Total Benchmark Scores, then list the top 5 companies by Total Benchmark Score descending.\n",
      "\n",
      "============================================================\n",
      "ðŸš€ Starting Multi-Agent Analysis\n",
      "ðŸ“ Query: Clean the Scoring sheet by removing any rows with missing or zero Total Benchmark Scores, then list the top 5 companies by Total Benchmark Score descending.\n",
      "============================================================\n",
      "\n",
      "ðŸŽ¯ Agents selected: data\n",
      "\n",
      "â–¶ï¸ Running DATA agent...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call', 'thought_signature'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " âœ… data completed\n",
      "\n",
      "============================================================\n",
      "âœ… Multi-Agent Analysis Complete\n",
      "============================================================\n",
      "\n",
      "I have cleaned the Scoring sheet by removing any rows with missing or zero Total Benchmark Scores.\n",
      "\n",
      "Here are the top 5 companies by Total Benchmark Score:\n",
      "\n",
      "1.  **Samsung Electronics Co. Ltd.** (South Korea): 61\n",
      "2.  **Hewlett Packard Enterprise Co. (HPE)** (United States): 53\n",
      "3.  **Cisco Systems Inc.** (United States): 51\n",
      "4.  **HP Inc.** (United States): 48\n",
      "5.  **Apple Inc.** (United States): 46\n"
     ]
    }
   ],
   "source": [
    "res = await smart_answer(\"Clean the Scoring sheet by removing any rows with missing or zero Total Benchmark Scores, then list the top 5 companies by Total Benchmark Score descending.\")\n",
    "print(res[\"final_report\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Analysis Agent: Computes statistical exploration and clustering (e.g., correlations via scipy, k-means in scikit-learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T19:23:53.997484Z",
     "iopub.status.busy": "2025-11-30T19:23:53.997201Z",
     "iopub.status.idle": "2025-11-30T19:23:58.798783Z",
     "shell.execute_reply": "2025-11-30T19:23:58.797703Z",
     "shell.execute_reply.started": "2025-11-30T19:23:53.997465Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ¤– Query: Compute the average Total Benchmark Score across all companies in the Scoring sheet, and the standard deviation.\n",
      "\n",
      "============================================================\n",
      "ðŸš€ Starting Multi-Agent Analysis\n",
      "ðŸ“ Query: Compute the average Total Benchmark Score across all companies in the Scoring sheet, and the standard deviation.\n",
      "============================================================\n",
      "\n",
      "ðŸŽ¯ Agents selected: data, analysis\n",
      "\n",
      "â–¶ï¸ Running DATA agent...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call', 'thought_signature'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š DATA AGENT: Loading KnowTheChain 2025 ICT Benchmark...\n",
      " âœ“ Loaded 47 rows from Sheet 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['thought_signature'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " âœ… data completed\n",
      "\n",
      "â–¶ï¸ Running ANALYSIS agent...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call', 'thought_signature'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " âœ… analysis completed\n",
      "\n",
      "============================================================\n",
      "âœ… Multi-Agent Analysis Complete\n",
      "============================================================\n",
      "\n",
      "The average Total Benchmark Score across all companies is 20.22, with a standard deviation of 14.23.\n"
     ]
    }
   ],
   "source": [
    "res = await smart_answer(\"Compute the average Total Benchmark Score across all companies in the Scoring sheet, and the standard deviation.\")\n",
    "print(res[\"final_report\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T19:23:43.382940Z",
     "iopub.status.busy": "2025-11-30T19:23:43.382629Z",
     "iopub.status.idle": "2025-11-30T19:23:46.503219Z",
     "shell.execute_reply": "2025-11-30T19:23:46.502494Z",
     "shell.execute_reply.started": "2025-11-30T19:23:43.382920Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ¤– Query: Calculate the correlation between Market Cap and Total Benchmark Score using the Scoring sheet data.\n",
      "\n",
      "============================================================\n",
      "ðŸš€ Starting Multi-Agent Analysis\n",
      "ðŸ“ Query: Calculate the correlation between Market Cap and Total Benchmark Score using the Scoring sheet data.\n",
      "============================================================\n",
      "\n",
      "ðŸŽ¯ Agents selected: data, analysis\n",
      "\n",
      "â–¶ï¸ Running DATA agent...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['thought_signature'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " âœ… data completed\n",
      "\n",
      "â–¶ï¸ Running ANALYSIS agent...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call', 'thought_signature'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " âœ… analysis completed\n",
      "\n",
      "============================================================\n",
      "âœ… Multi-Agent Analysis Complete\n",
      "============================================================\n",
      "\n",
      "The correlation between Market Cap and Total Benchmark Score is 0.323, with a p-value of 0.0305. This indicates a statistically significant positive correlation.\n"
     ]
    }
   ],
   "source": [
    "res = await smart_answer(\"Calculate the correlation between Market Cap and Total Benchmark Score using the Scoring sheet data.\")\n",
    "print(res[\"final_report\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T19:23:30.564807Z",
     "iopub.status.busy": "2025-11-30T19:23:30.563994Z",
     "iopub.status.idle": "2025-11-30T19:23:36.120406Z",
     "shell.execute_reply": "2025-11-30T19:23:36.119682Z",
     "shell.execute_reply.started": "2025-11-30T19:23:30.564777Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ¤– Query: Compute the median score for each theme (e.g., Commitment and Governance) across North American companies.\n",
      "\n",
      "============================================================\n",
      "ðŸš€ Starting Multi-Agent Analysis\n",
      "ðŸ“ Query: Compute the median score for each theme (e.g., Commitment and Governance) across North American companies.\n",
      "============================================================\n",
      "\n",
      "ðŸŽ¯ Agents selected: analysis, data\n",
      "\n",
      "â–¶ï¸ Running ANALYSIS agent...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call', 'thought_signature'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " âœ… analysis completed\n",
      "\n",
      "â–¶ï¸ Running DATA agent...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['thought_signature'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " âœ… data completed\n",
      "\n",
      "============================================================\n",
      "âœ… Multi-Agent Analysis Complete\n",
      "============================================================\n",
      "\n",
      "The median scores for each theme across North American companies are as follows:\n",
      "\n",
      "*   **Commitment and Governance:** 54.5\n",
      "*   **Enabling Workers:** 3\n",
      "*   **Monitoring:** 7.5\n",
      "*   **Purchasing Practices:** 0\n",
      "*   **Recruitment:** 8\n",
      "*   **Remedy:** 8\n",
      "*   **Total Benchmark:** 19\n",
      "*   **Traceability Risk:** 20\n",
      "\n",
      "These medians are based on data from 22 companies in North America.\n"
     ]
    }
   ],
   "source": [
    "res = await smart_answer(\"Compute the median score for each theme (e.g., Commitment and Governance) across North American companies.\")\n",
    "print(res[\"final_report\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Research Agent: Integrates external data via web searches (e.g.,ILO stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T19:28:48.653223Z",
     "iopub.status.busy": "2025-11-30T19:28:48.652882Z",
     "iopub.status.idle": "2025-11-30T19:28:55.429506Z",
     "shell.execute_reply": "2025-11-30T19:28:55.428574Z",
     "shell.execute_reply.started": "2025-11-30T19:28:48.653200Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ¤– Query: Fetch external data on global average market cap for ICT semiconductors and compare to KTC dataset average.\n",
      "\n",
      "============================================================\n",
      "ðŸš€ Starting Multi-Agent Analysis\n",
      "ðŸ“ Query: Fetch external data on global average market cap for ICT semiconductors and compare to KTC dataset average.\n",
      "============================================================\n",
      "\n",
      "ðŸŽ¯ Agents selected: data, analysis, research\n",
      "\n",
      "â–¶ï¸ Running DATA agent...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['thought_signature'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " âœ… data completed\n",
      "\n",
      "â–¶ï¸ Running ANALYSIS agent...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['thought_signature'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " âœ… analysis completed\n",
      "\n",
      "â–¶ï¸ Running RESEARCH agent...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['thought_signature'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " âœ… research completed\n",
      "\n",
      "============================================================\n",
      "âœ… Multi-Agent Analysis Complete\n",
      "============================================================\n",
      "\n",
      "I cannot directly fetch external data on global average market cap for ICT semiconductors or compare it to the KTC dataset average, as this functionality is beyond my current capabilities. My tools do not support accessing external market data or calculating market capitalization averages from the loaded KTC data.\n"
     ]
    }
   ],
   "source": [
    "res = await smart_answer(\"Fetch external data on global average market cap for ICT semiconductors and compare to KTC dataset average.\")\n",
    "print(res[\"final_report\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Prediction Agent: Models score trajectories and project improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T19:29:25.062156Z",
     "iopub.status.busy": "2025-11-30T19:29:25.061839Z",
     "iopub.status.idle": "2025-11-30T19:29:31.926705Z",
     "shell.execute_reply": "2025-11-30T19:29:31.925820Z",
     "shell.execute_reply.started": "2025-11-30T19:29:25.062136Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ¤– Query: Based on historical ranks (e.g., Amazon 2022 rank 8, 2025 rank 10), predict Amazon's 2027 rank if it improves Remedy by 10 points.\n",
      "\n",
      "============================================================\n",
      "ðŸš€ Starting Multi-Agent Analysis\n",
      "ðŸ“ Query: Based on historical ranks (e.g., Amazon 2022 rank 8, 2025 rank 10), predict Amazon's 2027 rank if it improves Remedy by 10 points.\n",
      "============================================================\n",
      "\n",
      "ðŸŽ¯ Agents selected: data, prediction\n",
      "\n",
      "â–¶ï¸ Running DATA agent...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['thought_signature'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " âœ… data completed\n",
      "\n",
      "â–¶ï¸ Running PREDICTION agent...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call', 'thought_signature'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
      "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call', 'thought_signature'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['thought_signature'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " âœ… prediction completed\n",
      "\n",
      "============================================================\n",
      "âœ… Multi-Agent Analysis Complete\n",
      "============================================================\n",
      "\n",
      "Based on the current model, if Amazon improves its Remedy score by 10 points, its rank is predicted to remain at 10. The tool does not provide a specific prediction for the year 2027, but rather the immediate impact on rank based on the described improvement.\n"
     ]
    }
   ],
   "source": [
    "res = await smart_answer(\"Based on historical ranks (e.g., Amazon 2022 rank 8, 2025 rank 10), predict Amazon's 2027 rank if it improves Remedy by 10 points.\")\n",
    "print(res[\"final_report\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T19:29:57.710838Z",
     "iopub.status.busy": "2025-11-30T19:29:57.710535Z",
     "iopub.status.idle": "2025-11-30T19:30:07.464225Z",
     "shell.execute_reply": "2025-11-30T19:30:07.463471Z",
     "shell.execute_reply.started": "2025-11-30T19:29:57.710818Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ¤– Query: Project the industry average Total Benchmark Score for 2027 if scores increase by 5% annually.\n",
      "\n",
      "============================================================\n",
      "ðŸš€ Starting Multi-Agent Analysis\n",
      "ðŸ“ Query: Project the industry average Total Benchmark Score for 2027 if scores increase by 5% annually.\n",
      "============================================================\n",
      "\n",
      "ðŸŽ¯ Agents selected: data, analysis, prediction\n",
      "\n",
      "â–¶ï¸ Running DATA agent...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['thought_signature'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " âœ… data completed\n",
      "\n",
      "â–¶ï¸ Running ANALYSIS agent...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call', 'thought_signature'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
      "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['thought_signature'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " âœ… analysis completed\n",
      "\n",
      "â–¶ï¸ Running PREDICTION agent...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call', 'thought_signature'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " âœ… prediction completed\n",
      "\n",
      "============================================================\n",
      "âœ… Multi-Agent Analysis Complete\n",
      "============================================================\n",
      "\n",
      "Based on the current industry average Total Benchmark Score of 20.22, if scores increase by 5% annually, the projected industry average Total Benchmark Score for 2027 would be approximately 22.30.\n"
     ]
    }
   ],
   "source": [
    "res = await smart_answer(\"Project the industry average Total Benchmark Score for 2027 if scores increase by 5% annually.\")\n",
    "print(res[\"final_report\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T19:30:27.576637Z",
     "iopub.status.busy": "2025-11-30T19:30:27.576280Z",
     "iopub.status.idle": "2025-11-30T19:30:38.007991Z",
     "shell.execute_reply": "2025-11-30T19:30:38.007201Z",
     "shell.execute_reply.started": "2025-11-30T19:30:27.576612Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ¤– Query: Model score improvement for Asian companies if they match North Americas avg Purchasing Practices (45).\n",
      "\n",
      "============================================================\n",
      "ðŸš€ Starting Multi-Agent Analysis\n",
      "ðŸ“ Query: Model score improvement for Asian companies if they match North Americas avg Purchasing Practices (45).\n",
      "============================================================\n",
      "\n",
      "ðŸŽ¯ Agents selected: prediction, data\n",
      "\n",
      "â–¶ï¸ Running PREDICTION agent...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call', 'thought_signature'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
      "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call', 'thought_signature'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
      "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call', 'thought_signature'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['thought_signature'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " âœ… prediction completed\n",
      "\n",
      "â–¶ï¸ Running DATA agent...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call', 'thought_signature'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š DATA AGENT: Loading KnowTheChain 2025 ICT Benchmark...\n",
      " âœ“ Loaded 47 rows from Sheet 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['thought_signature'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " âœ… data completed\n",
      "\n",
      "============================================================\n",
      "âœ… Multi-Agent Analysis Complete\n",
      "============================================================\n",
      "\n",
      "If Asian companies match North America's average Purchasing Practices score of 45, their Total Benchmark score is predicted to improve by an average of 3.15 points. This projection is based on 16 companies in Asia.\n"
     ]
    }
   ],
   "source": [
    "res = await smart_answer(\"Model score improvement for Asian companies if they match North Americas avg Purchasing Practices (45).\")\n",
    "print(res[\"final_report\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Synthesis Agent: Compiles insights with ethical checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T19:32:40.240628Z",
     "iopub.status.busy": "2025-11-30T19:32:40.240254Z",
     "iopub.status.idle": "2025-11-30T19:32:50.966631Z",
     "shell.execute_reply": "2025-11-30T19:32:50.965872Z",
     "shell.execute_reply.started": "2025-11-30T19:32:40.240603Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ¤– Query: Synthesise key insights on top 5 companies strengths and weaknesses from Scoring sheet.\n",
      "\n",
      "============================================================\n",
      "ðŸš€ Starting Multi-Agent Analysis\n",
      "ðŸ“ Query: Synthesise key insights on top 5 companies strengths and weaknesses from Scoring sheet.\n",
      "============================================================\n",
      "\n",
      "ðŸŽ¯ Agents selected: data\n",
      "\n",
      "â–¶ï¸ Running DATA agent...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call', 'thought_signature'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š DATA AGENT: Loading KnowTheChain 2025 ICT Benchmark...\n",
      " âœ“ Loaded 47 rows from Sheet 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call', 'thought_signature'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
      "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['thought_signature'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " âœ… data completed\n",
      "\n",
      "============================================================\n",
      "âœ… Multi-Agent Analysis Complete\n",
      "============================================================\n",
      "\n",
      "The top 5 companies from the Scoring sheet are:\n",
      "\n",
      "1.  **Samsung Electronics Co. Ltd.** (Total Benchmark: 61)\n",
      "2.  **Hewlett Packard Enterprise Co. (HPE)** (Total Benchmark: 53)\n",
      "3.  **Cisco Systems Inc.** (Total Benchmark: 51)\n",
      "4.  **HP Inc.** (Total Benchmark: 48)\n",
      "5.  **Apple Inc.** (Total Benchmark: 46)\n",
      "\n",
      "To synthesize key insights on their strengths and weaknesses, I would need to analyze their scores across different themes (e.g., Commitment & Governance, Traceability & Risk, Purchasing Practices, etc.), not just their overall Total Benchmark score.\n",
      "\n",
      "While I can retrieve detailed scoring information, I am not designed to *synthesize* or *interpret* this data into \"key insights\" about strengths and weaknesses directly. I can, however, provide you with the detailed scores for each of these companies across the available themes, which you could then use for your analysis.\n",
      "\n",
      "Would you like me to load the detailed research for these companies so you can see their scores across all themes?\n"
     ]
    }
   ],
   "source": [
    "res = await smart_answer(\"Synthesise key insights on top 5 companies strengths and weaknesses from Scoring sheet.\")\n",
    "print(res[\"final_report\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Ethics Agent: Evaluates biases using principal-agent theory, addressing risks like amplification in SCM decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T19:33:14.469027Z",
     "iopub.status.busy": "2025-11-30T19:33:14.468681Z",
     "iopub.status.idle": "2025-11-30T19:33:20.823084Z",
     "shell.execute_reply": "2025-11-30T19:33:20.822112Z",
     "shell.execute_reply.started": "2025-11-30T19:33:14.469004Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ¤– Query: Evaluate potential biases in the Scoring sheet data, using principal-agent theory.\n",
      "\n",
      "============================================================\n",
      "ðŸš€ Starting Multi-Agent Analysis\n",
      "ðŸ“ Query: Evaluate potential biases in the Scoring sheet data, using principal-agent theory.\n",
      "============================================================\n",
      "\n",
      "ðŸŽ¯ Agents selected: data, ethics\n",
      "\n",
      "â–¶ï¸ Running DATA agent...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['thought_signature'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " âœ… data completed\n",
      "\n",
      "â–¶ï¸ Running ETHICS agent...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call', 'thought_signature'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " âœ… ethics completed\n",
      "\n",
      "============================================================\n",
      "âœ… Multi-Agent Analysis Complete\n",
      "============================================================\n",
      "\n",
      "The analysis of potential geographic bias in the Scoring sheet data reveals that regional differences are within an acceptable range.\n",
      "\n",
      "Here's a summary of the findings:\n",
      "*   **Bias Detected:** No significant bias was detected.\n",
      "*   **Regional Averages:**\n",
      "    *   North America: 23.68\n",
      "    *   Europe: 22.71\n",
      "    *   Asia: 14.38\n",
      "*   **Maximum Region:** North America (23.68)\n",
      "*   **Minimum Region:** Asia (14.38)\n",
      "*   **Gap:** The difference between the maximum and minimum regional average is 9.3.\n",
      "*   **Variance Ratio:** 0.783\n",
      "*   **Recommendation:** Regional differences are considered to be within an acceptable range.\n",
      "\n",
      "While this assessment addresses regional bias, it does not specifically apply principal-agent theory. Principal-agent theory would require an analysis of how information asymmetry, conflicting interests, and monitoring costs between the principals (e.g., stakeholders concerned with fair scoring) and agents (e.g., those responsible for data collection and scoring) might introduce or amplify biases in the scoring sheet data. However, based on the available tools, a direct evaluation using this theory is not possible.\n"
     ]
    }
   ],
   "source": [
    "res = await smart_answer(\"Evaluate potential biases in the Scoring sheet data, using principal-agent theory.\")\n",
    "print(res[\"final_report\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 14484960,
     "sourceId": 121144,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
